% https://www.overleaf.com/read/jydxqkkkskzp
% https://github.com/MCG-NKU/NSFC-LaTex
% by Ming-Ming Cheng https://mmcheng.net

\documentclass[12pt]{article}
\usepackage[UTF8]{ctex}
\usepackage{nsfc}

\newcommand{\cmm}[1]{\textcolor[rgb]{0,0.6,0}{CMM: #1}}
\newcommand{\todo}[1]{{\textcolor{red}{\bf [#1]}}}
\newcommand{\myPara}[1]{\paragraph{#1：}}

\graphicspath{{figures/}}


\begin{document}



%%%%%%%%% TITLE

\title{报告正文：危险品智能装配中的高精度6D姿态估计方法研究}

\maketitle

\ContentDes{（一）立项依据与研究内容（建议8000字以下）：}


\NsfcSection{1}{项目的立项依据}{
（研究意义、国内外研究现状及发展动态分析，需结合科学研究发展趋势来论述科学意义；
或结合国民经济和社会发展中迫切需要解决的关键科技问题来论述其应用前景。
附主要参考文献目录）；}




\subsection{研究意义}

智能制造是推进我国制造强国战略的主攻方向，危险设备智能装配是其中重要的落地场景。而装配零部件六自由度姿态估计是危险设备智能装配中的关键问题。


\subsection{国内外相关工作}

针对火药等易燃易爆非稳态危险设备装配零部件六自由度姿态估计，研究者相继设计了不同的姿态估计算法以实现自动化装配。相对人工手动装配，该类装配方案取得了长足的进步，但随着危险设备装配向复杂环境延伸，当前危险设备装配零部件六自由度姿态估计存在下述不足：

首先，RGB，Depth和光谱数据之间的融合方面存在以下问题：(1) 在数据表达上，多模态具有互补性，同时数据的异构性也带来了重复表达特征的问题，造成数据表示的冗余。(2) 在数据提取上，不同模态信息需要不同的处理方式，如何合理的从中提炼有效信息，也为数据提取带来了难度和增加工作量。(3) 在信息对齐上，不同信息的表达方式不同，需要从特性中找共性，才能在同一个系统中将二维图像、三维模型以及深度点云信息对齐。此外，因为数据样本中存在噪声，在噪声的干扰下，很显然是没有办法做到精准对齐的。(4) 在信息融合上，如何高效结合多个特征信息，而不是简单的叠加组合。 

其次，针对复杂环境下6D位姿估计问题的目标检测方法存在以下问题：(1) 在信息获取方面，单目、双目RGB相机在光照过强或过弱的情况下无法提供充分有效的目标信息,而点云特征在实际应用中难以保证检测的高精度与高效率。(2) 在可视化建模方面，传统的为所有目标注释分割掩膜(annotating segmentation masks)的方法非常复杂，并且在受到场景元素遮挡时该方法的可扩展性(scalability)将会受到限制。(3) 在特征提取方面，目标检测器专注于从真实边界框的中心区域抽取样本来预测边界参数，使其无法应对中心区域被其它目标或场景元素遮挡的情况。(4) 在推理预测方面，采用非最大抑制处理多个预测框的方法依赖于边框中心的小区域信息，忽视了刚体目标可见部分提供的信息。

再者，工业零部件呈现出弱纹理以及无纹理的特点，根据纹理特征进行6D姿态的估计是不切实际的。

针对上述问题，本课题研究复杂环境下适用的目标六自由度姿态估计算法以辅助危险设备智能装配，用深度学习的方法解决关键技术细节。

\subsubsection{面向危险设备智能装配零部件六自由度姿态估计的多源融合表征的研究进展（刘志强）}

一般而言，多模态数据包含：图像数据、激光雷达数据、毫米波雷达数据、双目深度数据等。绝大多数传统研究将图像数据和激光雷达数据进行融合来提升3D目标检测精度。依据融合位置的不同，基于图像数据和激光雷达数据融合的多模态检测可以分为三类：第一类，前融合方法。该方法首先利用2D检测或分割网络从图像中提取知识，然后将图像知识传递给点云，最后将增强后的点云反馈给基于激光雷达的点云3D目标检测器。例如，文献\cite{Peng2019}提出了一种区域级融合方法生成3D视锥，并将其应用于激光雷达点云来减小目标候选区域的范围。华南理工大学的贾奎课题组在此基础上将3D视锥划分为网格单元，并在网格单元上应用卷积网络进行3D检测\cite{Hu2019}。文献[3]利用基于图像的语义分割来增强输入点云，将增强后的点云送入激光雷达检测器，获得了较好的检测结果。除了语义分割，也有一些研究利用图像中的其他信息，例如使用深度图像来增强稀疏的3D点云 [4]。尽管前融合方法通过有效的预处理提高了检测性能，但是这种以顺序的方式执行多模态融合和3D目标检测的过程带来了不可忽视的时间成本；第二类，中融合方法。该方法在基于激光雷达的3D目标检测器的中间阶段融合图像和激光雷达特征。例如，在基于网格的检测网络骨干的中间层中，使用连续卷积(Continuous convolutions)\cite{Park2019,Su2022}、混合体素特征编码(Hybrid voxel feature encoding)[7]和变换(Transformer)[8]网络等融合算子进行多模态融合。文献[9]提出了一种多视角融合方法，在提议生成(Proposal generation)阶段进行多通道特征融合。这类方法建议对多模态表示进行更深层的融合，并产生更高质量的3D框，但在由于相机和激光雷达的特征本质上是异构的，因此在融合机制和视角对齐方面仍存在问题。第三类，后融合方法。该方法将图像和激光雷达数据分别进行单独处理，并将输出的2D和3D框融合[9]。与前两类融合方法相比，该方法不仅可以更好的利用每个通道现有的网络，而且还避免了中间特征或输入点云上的复杂交互，拥有较高的执行效率。但是，由于输出结果不依赖于图像和激光雷达传感器的深度特征，这种方法无法利用不同模式的丰富语义信息，直到近几年才受到研究者关注[10]。

\subsubsection{针对危险设备智能装配零部件3D目标检测的研究进展（王庆元）}
\textbf{2D目标检测的研究进展}

目前一些先进的6D姿态估计方法[11, 12, 13]通常需要使用2D目标检测器来为场景中的所有目标提取准确的2D边界框，获得感兴趣区域(Region of Interest, RoI)。现有的2D目标检测主要遵循两种策略：二阶段检测和一阶段检测。二阶段检测器首先使用区域建议网络(Region Proposal Network, RPN)来生成边界框候选，然后用分类和细化网络处理候选边界框以去除假正(False Positive, FP)样本并调整边界框的位置和大小[14, 15]。这种策略具有较高的检测精度，但是在实际应用中需要较高的成本并且计算效率较低。一阶段检测器在编码器最后的特征图中的每个空间位置用一组预定义的锚点(Anchor)代替区域建议网络[16]，取得了较高的计算效率。但是，预定义的锚点中存在大量的负(Negative)样本导致检测性能较差。文献[17]通过焦点损失(Focal Loss)在一定程度上解决了样本数量不平衡问题，但是仍然没有达到二阶段检测器的精度。文献[18]在一阶段检测器中对正候选样本进行采样，有效地解决了负样本过多的问题。许多最新的检测方法遵循类似的策略[19, 20]，实现了比二阶段检测方法更加准确和高效的目标检测。这些方法在标准的目标检测基准测试中表现出了很好的性能，但是它们在6D位姿估计的基准测试中通常会受到严重遮挡等复杂因素的影响出现检测缺失或不准确的情况。

\textbf{3D目标检测的研究进展}
主流3D目标检测方法依据数据模态可以分为三类：第一类，基于单目图像。由于相机能够提供充分的2D图像信息，一些研究使用发展成熟的2D目标检测的方法来实现3D目标检测，并且以较低的成本获得了令人满意的性能[21]。为了解决单目相机下三维信息无法获取的问题，文献[22]采用了预先训练深度估计网络的方法生成深度图像，以获取深度感知特征来提高检测性能。一些研究尝试引入预先训练的辅助网络来学习图像中目标形状和几何一致性等先验信息来帮助准确定位3D目标。例如，文献[23]从CAD模型中学习低维形状参数来获取特定类别的先验形状特征，提出一种渲染比较方法来学习3D目标的参数。然而，这种预先训练的网络模型需要不可忽视的注释成本，并且存在数据泛化问题。第二类，基于双目立体图像。与单目图像相比，双目立体图像可以提供准确的深度信息以及丰富的图像语义细节。例如，文献[24]提出了一种新颖的立体检测框架来为成对的图像生成左右兴趣区域(Region of interests, RoIs)，然后融合RoIs来估计3D目标参数。文献[25]将2D深度图转换为3D激光雷达点云，并且将3D深度估计网络与3D目标检测网络结合，使得整个网络能够端到端的训练。虽然双目立体图像能够提供更精确的目标定位信息，但是该方法仍然无法在光线不足、严重遮挡等恶劣条件下工作。第三类，基于点云。点云数据是一种稀疏且不规则的3D表示，因其能够提供丰富的环境信息以及物体的几何特征，在3D目标检测领域受到广泛关注。根据模型的检测过程，可以分为单阶段检测和双阶段检测。单阶段检测是指从原始的点云数据中获取特征并直接预测目标的3D边界框。研究人员使用不同的数据预处理技术来处理稀疏的点云数据以便后续的主干网络获取点云特征，例如基于点的3D目标检测[26]、基于体素的3D目标检测[27]、基于图的3D目标检测[28]等。双阶段检测方法是指在第一阶段生成3D候选目标，在第二阶段对第一阶段中生成的候选目标进行细化。文献[29]是一项开创性研究，该方法在第一阶段将点云划分为球形体素进而提取点云特征并生成3D目标提议，在第二阶段输入点云采样中的关键点，利用新的点算子来获得更加丰富的特征。文献[30]在此基础上使用新型的运算方法来改进第二阶段的模块。然而在实际应用中，如何保证高精度3D目标检测同时保持高效率仍然是该研究领域面临的一个挑战。

上述研究均取得了被领域认可的检测结果，为本课题研究提供了理论基础，但是面对复杂环境中6D位姿估计问题，仍然进一步完善来获得更加精确的目标检测：首先，单目、双目相机在光照过强或过弱的情况下无法提供充分有效的目标信息；其次，传统的为所有目标注释分割掩膜的可视化建模方法非常复杂，并且在受到场景元素遮挡时该方法的可扩展性将会受到限制；再者，单阶段2D目标检测忽视了刚体目标可见部分提供的信息，专注于从真实边界框的中心区域抽取样本来预测边界参数使其无法应对中心区域被其它目标或场景元素遮挡的情况；另外，在推理预测过程中采用非最大抑制处理多个预测框的方法依赖于边框中心的小区域信息，忽视了刚体目标可见部分提供的信息。本课题尝试融合光谱特征来提供准确的目标信息，并有效利用刚体可见部分的特征来对3D边界框进行预测。

\subsubsection{针对危险设备智能装配零部件目标六自由度姿态估计研究进展（海洋）}

基于CNN的6D姿态估计方法可以被划分为以下几种：（1）基于2D-3D对应关系，进一步结合RANSAC-PnP来求解物体的6D姿态。该类方法由于其在网络学习过程可以提供显式明确的监督信号，在6D姿态估计领域中显示出了优良的性能。其中该类方法可以进一步划分为基于关键点的方法，即稀疏的3D-to-2D对应关系[31-33]，以及基于稠密2D-to-3D对应关系的方法[34-36]。文献[31]提出通过物体可见掩膜上的每个点预测到关键点投影的方向，进而通过投票的方法来确定关键点投影的位置。文献[37] 对物体的三维模型进行分层编码，预测每个物体可见掩膜上每个像素对应3D点的编码，将回归问题转化为分类问题，在大多数6D姿态估计基准上创造了新的记录。（2）直接回归6D姿态的方法。该类方法端到端地估计目标物体的6D姿态。然而由于6D姿态回归的歧义性以及无法为网络提供足够直接的监督信号，该类方法在性能上劣于基于2D-3D对应关系的方法。文献[38]提出使用稠密的2D-to-3D对应关系作为中间表示，来协助6D姿态的回归。文献[39] 提出使用PointNet神经网络来近似作用在稀疏的3D-to-2D对应关系的PnP。近期，基于CNN的6D姿态修正方法获得了越来越多学者的关注，该类方法的目的是使用一个单独的网络来修正姿态估计网络估计的不精确的6D姿态。由于其迭代优化的范式，该类方法可以在性能和精度方面进行权衡。文献[40]证明基于光流的6D姿态修正方法具有优良的领域泛化能力，即在合成图像上进行训练，即可泛化到真实图像上。文献[41]进一步结合多视图与可微分捆绑调整层来使姿态修正网络可以利用深度图像来训练与测试。

{
\bibliographystyle{ieee}
\bibliography{nsfc_sr}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NsfcSection{2}{项目的研究内容、研究目标，以及拟解决的关键科学问题}{
（此部分为重点阐述内容）；}

\subsection{研究目标}
本项目针对危险设备智能装配零部件六维位姿估计中的问题和困难，使用三维视觉传感器和光谱仪获取目标RGB数据、Depth数据以及光谱数据，将多模态学习引入到危险设备智能装配零部件的六维位姿估计中，展开理论方法和应用研究。具体目标包括：(1)面向危险设备智能装配零部件的多源融合表征（刘志强）(2)针对危险设备智能装配零部件复杂环境下的3D目标检测（王庆元）(3)面向危险设备智能装配基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计（海洋）



\subsection{研究内容}

为了解决同类研究在真实复杂环境下暴露的不足，本课题组研究涉及危险设备智能装配环境下基于几何特征引导的目标六自由度姿态估计问题，并融入多源融合表征以及多模态3D目标检测框架。研究内容可提炼为三个方面，它们互相联系构成完整的面向危险设备智能装配零部件的六自由度姿态估计系统。

\textbf{A、面向危险设备智能装配零部件的多源融合表征（刘志强）}
针对危险设备智能装配零部件的多源融合表征存在的问题和挑战，以融合方案对位姿估计优化后的精度为评价依据，研究多模态信息的表达和特征提取。研究该过程包含两个细节问题：（1）针对目标数据的差异性以及不同模态数据对于姿态估计任务的重要性，解决姿态估计问题中多种信息表达的关键问题，制定高效针对性的目标特征提取策略；（2）研究神经网络下多模态特征的信息对齐和融合问题，提出可行的多模态信息融合方式。

\textbf{B、针对危险设备智能装配零部件复杂环境下的3D目标检测（王庆元）}
针对复杂场景下目标检测算法存在的问题和挑战，研究利用刚性目标为边框预测提供的有效信息来提高检测进度。首先，研究利用目标边界框中的所有像素来指导生成可见性(visibility)掩膜；然后，使用可见性掩膜指导训练期间对候选目标的抽样，使得网络由所有可见部分监督并且丢弃遮挡部分；最后，研究融合所有高可靠性候选局部预测，以产生更加鲁棒的检测结果。

\textbf{面向工业机械臂抓取基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计（海洋）}
基于检测-估计-修正的6D姿态估计框架显示出了优良的性能，被近期提出的大多数方法所采用。采用一个单独的6D姿态修正网络可以放松对前一级估计网络的要求。同时基于CNN的6D姿态修正网络的输入为目标图像与在前一级估计网络得到的粗糙姿态下的渲染图像，这样的输入可以减轻姿态修正网络对数据的需求。目前大多数方法通过估计目标图像与渲染图像之间的对应关系来求解目标图像中物体的6D姿态。然而这种方法很强地依赖于物体的纹理特征，而对于工业零部件，大部分物体呈现出无纹理或者弱纹理的特征，因此这种方法应用于工业零部件上会有很强的局限性。因此对于无纹理或者弱纹理的工业零部件，物体的几何特征需要被有效地利用，然而现有的姿态修正方法很少探索物体的几何特征。
本课题拟提出一个基于物体几何特征的6D姿态修正方法。在现有的基于纹理特征的对应关系估计网络中，如RAFT，本文通过利用点云特征提取网络，显式地将物体的几何特征进行编码，作为对应关系估计网络的参考信息。在迭代优化过程中，对应关系估计网络全局地通过可微姿态求解层来修正在上一步迭代过程中求解得到的姿态。具体来说，将物体的几何特征在上一步估计得到的姿态下进行投影，同时将物体的几何信息引入目标图像与渲染图像的特征匹配过程中，结合特征匹配结果与投影得到的几何特征来全局地求解更新之后的目标物体的6D姿态。



\subsection{拟解决的关键科学问题}

\textbf{A. 面向危险设备智能装配零部件的多源融合表征（刘志强）}
如立论部分所述，在姿态估计中加入多种信息，理论上是有利于最终结果的评估，但是在实际处理中，各种模态信息具有各自的信息特征，将多种信息有效的融合则是高效解决问题的关键。针对危险设备智能装配环境，引入光谱数据区分火工品装配危险区与非危险区。针对目标数据的差异性，制定高效鲁棒的目标特征提取策略，同时考虑多模态特征的信息对齐和融合。

\textbf{B. 针对危险设备智能装配零部件复杂环境下的3D目标检测（王庆元）}
如立论部分所述，复杂场景中的目标存在严重的遮挡问题，这使得专注于学习从真实边界框中心区域抽取的样本来预测边界参数的方法就不再适用，有效利用刚性目标对象的可见部分来获取目标边框的可靠估计是解决该问题的关键。本课题利用边界框中所有像素信息来获得目标可见部分的掩膜，取代了传统复杂的标注分割掩膜的可视化方法。针对目标中心遮挡情况下无法获取有效样本问题，在推理过程中融合所有高可靠性候选局部预测，以获得更准确的检测结果。

\textbf{C. 面向危险设备智能装配基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计（海洋）}
如立论部分所述，对于弱纹理的物体，当前基于纹理特征的6D姿态修正方法是不适用的。课题组前期对此进行了实验分析。如图所示，对于图中的弱纹理物体，基于纹理特征估计的渲染图像到目标图像之间的2D对应关系较差，无法保持物体的形状，导致最终通过RANSAC-PnP估计得到的姿态精度较差。尽管图中所示的物体不属于工业零部件，但是其无纹理的特征与工业零部件是相同的。通过该例子引申出工业零部件的6D姿态修正难点所在，由于工业零部件的弱/无纹理特征，在对零部件拍摄得到的图像中，大部分物体上的 像素呈现出一致的纹理特征，不具有区分度。在这种情况下，通过上下文信息以及几何特征感知像素在物体几何模型上的位置，进而估计2D对应关系是可行的解决方案。


\NsfcSection{3}{拟采取的研究方案及可行性分析}{
（包括研究方法、技术路线、实验手段、关键技术等说明）；}


\subsection{拟采取的技术路线}
本课题研究的面向危险设备智能装配零部件六自由度姿态估计的总体方案示于图\ref{fig:sys_arch}。其中涉及的三个关键问题的详细方案和可行性分析分别阐述如下：

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{sys_arch.png}
    \end{overpic}
    \caption{本课题研究方案的总体框架
    }\label{fig:sys_arch}
\end{figure}

\textbf{A.面向危险设备智能装配零部件的多源融合表征设计方案及可行性分析（刘志强）}

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{feature_biflow_fusion.png}
    \end{overpic}
    \caption{特征级双向全流多源融合
    }\label{fig:feature_biflow_fusion}
\end{figure}

输入部分为三种模态数据：2D彩色图、2.5D深度图和光谱图像。根据数据表达的意义采取不同的处理手段：对于2D彩色图像，表达目标实时纹理颜色，直接提取二维语义特征；对于2.5D深度图片，表达目标上各点到相机的距离，表达为三维点云更充分保留空间信息，提取空间特征；对于光谱图像，在危险设备装配环境中能更加有效识别目标，作为辅助数据优化结果。整个目标姿态估计网络框架分为四个部分：融合UV输入、特征提取、特征共享和特征融合。

\textbf{B. 针对危险设备智能装配零部件复杂环境下的3D目标检测网络设计方案及可行性分析}（王庆元）
\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{visable_guided_3d_detect.jpg}
    \end{overpic}
    \caption{可见性引导的3D目标检测流程图
    }\label{fig:visable_guided_3d_detect}
\end{figure}

该关键问题的研究重点着眼于复杂环境下刚性目标可见部分引导检测。首先，对真实刚性目标边界框进行裁剪来得到图像块，依据刚性目标的可见部分与边界部分像素之间的显著差异性，设计一种可见性距离函数并计算图像块内所有像素与边界距离来构建目标的可见性掩膜；然后使用主干网路对融合特征进行特征提取来生成特征图，依据每个特征单元对应所有像素的可见性距离来计算特征单元的可见性分数，设置可见性分数阈值对所有的特征单元进行筛选采样获得候选正单元；最后，研究特征单元预测框的置信度，并根据置信度融合所有的候选预测以获得更加鲁棒和精确的检测的结果。
一阶段目标检测器通常使用特征金字塔(FPN)输出多种比例的特征图，将特征图中的特征向量作为训练样本进行分类和回归处理。在检测训练期间，需要为每个注释的目标实例定义正样本和负样本，并使用正样本回归目标实例的边界框参数。因此这一框架的关键是在训练期间有效的选择正样本。本课题的前期预研中，在常规的目标数据集(COCO)和典型的6D目标位姿数据集(YCB-V)上，将真实目标掩膜内随机采样一定数量的正单元策略与两种基准策略进行评估对比，包括从真实边界框(FCOSv2)的中心区域采样固定数量的正单元，以及跨所有金字塔特征层级的自适应中心采样策略(ATSS)。图4显示了使用这些采样策略获得的不同局部预测的平均测试精度。对于常规的COCO数据集，无论在训练期间使用哪种采样策略，精度都会随着距离的增加而恶化。这因为COCO数据集中包括许多非刚体目标和具有相同目标类型的各种实例，使得基于中心区域采样的方法也可以获得可靠的边框预测结果。对于YCB-V数据集，由于大多数非中心区域特征没有参与训练，基于中心区域采样的策略的准确性也随着距离的增加而快速下降。然而，由于YCB-V目标的刚性，可见性引导(visibility guided)策略即使是对于那些远离中心区域的预测，也能取得更准确的局部预测。本课题将把该方法从2D边界框回归拓展到3D目标检测领域。

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{rigid_detect_analysis.png}
    \end{overpic}
    \caption{刚性检测分析
    }\label{fig:rigid_detect_analysis}
\end{figure}

我们展示了不同抽样策略的检验精度。在典型的一般对象数据集(COCO[29])和典型的6D目标位姿数据集(YCB-V)上的训练期间，不同的局部预测。图中显示了FCOSv2(Center)、ATSS(Center+)的结果，以及利用真实掩膜中的所有候选框(Visible)策略的结果。水平轴表示局部预测到边框中心的归一化距离。

\textbf{C. 面向危险设备智能装配基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计方案及可行性分析（海洋）}

本方案的关键问题在于对针对目标物体估计得到的不精确的6D姿态进行修正。利用该6D姿态在图像对物体进行定位，进而对目标物体所在的区域进行放大，同时减轻其他物体以及背景对姿态修正的影响。该方案包括几个部分，首先利用卷积神经网络对在不精确的6D姿态下渲染的初始图像以及目标图像进行特征提取，接下来迭代地进行6D姿态的修正，借助于RAFT中提出的相关性查找组件，计算在估计得到的初始图像与目标图像之间密集的2D对应关系（即光流）下的相似度。利用该相似度，本方案进一步估计对该对应关系的修正。为了将物体的几何信息引入特征匹配过程，本方案根据修正之后的密集对应关系，通过姿态估计子网络直接回归6D姿态。接下来本方案将物体的CAD模型在该6D姿态下进行投影，得到几何信息驱动的对应关系，以作用于下一次迭代中的相关性查找中。

随着RAFT的提出，基于RAFT中迭代修正框架在许多领域展现出了巨大的潜力。文献[10]进一步成功地将RAFT应用到6D姿态修正领域，并展现出了优良的性能。这是本项目分析可行性的重要依据。本项目在RAFT的基础上，将物体的几何信息引入特征匹配过程，使得匹配过程结合纹理信息与几何信息，不仅可以减小匹配过程中的搜索范围，同时几何信息的引入使得无/弱纹理物体的姿态修正更加鲁棒。

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{geo_guided_6D_refine.png}
    \end{overpic}
    \caption{基于几何特征引导的6D姿态修正框架
    }\label{fig:geo_guided_6D_refine}
\end{figure}

\subsection{可行性分析}


\NsfcSection{4}{本项目的特色与创新之处；}{}

\textbf{A. 多源融合表征的特色与创新之处（刘志强）}
一方面，通过引入额外的UV数据保持深度图点特征三维空间位置的一致性，由此可通过参数共享的神经网络架构，在RGB和Depth特征提取阶段，通过全流双向融合各自互补特征。另一方面，通过引入光谱图像，针对危险设备智能装配零部件不同地光谱特征响应，可以更准确和鲁棒地提取装配零部件实例级目标特征。

\textbf{B. 3D目标检测的特色与创新之处（王庆元）}
引入刚性感知检测方法，提出了一种新颖的可见性图来取代掩膜标注，并利用可见性图指导网络在训练期间对可见区域样本采样；考虑同一实例的不同尺度给检测带来的影响，引入尺度感知器来对融合特征不同尺度的特征图进行处理，提高了模型的特征提取与利用能力，并结合2D特征图采样结果来指导3D检测框回归；融合所有高于置信度阈值的候选局部预测，从而产生更鲁棒的检测结果。

\textbf{C. 基于目标几何特征引导的6D位姿估计的特色与创新之处（海洋）}
通过可微相对姿态求解层逐步优化估计的6D姿态，并将几何信息引入特征匹配过程，使得匹配得到的特征与物体的几何形状一致。进一步显式地使用深度神经网络编码几何特征作为姿态修正网络的参考信息。

\NsfcSection{5}{年度研究计划及预期研究结果}{
（包括拟组织的重要学术交流活动、国际合作与交流计划等）。}

\subsection{年度研究计划}

本课题研究期限从2023年1月到2026年12月。年度计划如下：
2023.01-2023.06		总体方案制定和论证
查阅计算机视觉领域和机器人领域国际知名期刊和会议的最新文献
开始对课题组的博士生和研究生进行python，pytorch以及三维视觉的入门培训
撰写算法设计书
2023.07-2023.12		三维视觉感知硬件平台搭建
搭建三维视觉硬件平台
搭建算法训练测试服务器平台，完善课题组原有服务器实验平台
2024.01-2024.06		多源融合表征建模研究
研究内容1：面向危险设备智能装配零部件的多源融合表征
拟参加国际会议学术交流一次，拟完成国际期刊论文1~2篇，专利1项
2024.07-2024.12		3D目标检测算法研究
研究内容2：针对危险设备智能装配零部件复杂环境下的3D目标检测
拟完成国际期刊论文1~2篇，专利1项
2025.01-2025.06		六自由度姿态估计算法研究
研究内容3：面向工业机械臂抓取基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计
拟完成国际期刊论文1~2篇，专利1项
2025.07-2025.12		各算法模块的优化
多源融合，目标检测和六维位姿估计算法形成模块，并在软硬件平台上进行验证和完善
拟完成国际期刊论文1~2篇
2026.01-2026.08		各算法模块的联合
多源融合，目标检测和六维位姿估计算法整合成完整的针对危险设备智能装配零部件目标六维位姿估计系统，并在软硬件平台上进行测试和完善
拟完成国际期刊论文1~2篇
2026.09-2026.12		准备课题评审
整理研究成果
撰写课题总结报告

\subsection{预期研究成果}

本课题将提出适用于危险设备智能装配零部件六自由度姿态估计算法。并基于该理论的研究成果，再国际学术期刊、国际学术会议和国内一级刊物上发表论文6-8篇；申请专利1-2项；培养博士研究生1-2名，硕士研究生3-5名。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ContentDes{（二）研究基础与工作条件}


\NsfcSection{1}{研究基础}{
（与本项目相关的研究工作积累和已取得的研究工作成绩）；}

\subsection{工作基础1}

在6D位姿估计方面，在前期工作中，课题组积累了大量公开数据集，同时在计算机视觉顶级会议ECCV 2022举办的BOP 6D位姿估计挑战赛中获得了最佳单模型奖项，提出的方法可以同时处理单目RGB以及RGB-D图像，可扩展性较强。

\subsection{工作基础2}



\subsection{研究工作获奖}


\NsfcSection{2}{工作条件}{
（包括已具备的实验条件，尚缺少的实验条件和拟解决的途径，
包括利用国家实验室、
国家重点实验室和部门重点实验室等研究基地的计划与落实情况）；}


\myPara{经费和硬件条件方面}我们


\myPara{人员方面}我们

\myPara{国内外合作方面} 我们


\NsfcSection{3}{正在承担的与本项目相关的科研项目情况}{
（申请人和项目组主要参与者正在承担的与本项目相关的科研项目情况，
包括国家自然科学基金的项目和国家其他科技计划项目，
要注明项目的名称和编号、经费来源、起止年月、与本项目的关系及负责的内容等）；}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ContentDes{（三） 其他需要说明的问题}



\NsfcSection{1}{}{
申请人同年申请不同类型的国家自然科学基金项目情况
（列明同年申请的其他项目的项目类型、项目名称信息，
并说明与本项目之间的区别与联系）。}


\NsfcSection{2}{}{
具有高级专业技术职务（职称）的申请人或者主要参与者是否存在
同年申请或者参与申请国家自然科学基金项目的单位不一致的情况；
如存在上述情况，列明所涉及人员的姓名，
申请或参与申请的其他项目的项目类型、项目名称、单位名称、
上述人员在该项目中是申请人还是参与者，并说明单位不一致原因。}



\NsfcSection{3}{}{
具有高级专业技术职务（职称）的申请人或者主要参与者是否具有
高级专业技术职务（职称）的申请人或者主要参与者是否存在与正
在承担的国家自然科学基金项目的单位不一致的情况；如存在上述情况，
列明所涉及人员的姓名，正在承担项目的批准号、项目类型、项目名称、
单位名称、起止年月，并说明单位不一致原因。}


\NsfcSection{4}{}{其他。}

无


\end{document}
