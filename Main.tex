% https://www.overleaf.com/read/jydxqkkkskzp
% https://github.com/MCG-NKU/NSFC-LaTex
% by Ming-Ming Cheng https://mmcheng.net

\documentclass[12pt]{article}
\usepackage[UTF8]{ctex}
\usepackage{nsfc}

\newcommand{\note}[1]{\textcolor[rgb]{0.6,0,0}{note: #1}}
\newcommand{\todo}[1]{{\textcolor{red}{\bf [#1]}}}
\newcommand{\myPara}[1]{\paragraph{#1：}}

\graphicspath{{figures/}}


\begin{document}



%%%%%%%%% TITLE

\title{报告正文：危化品智能装配中的高精度6D姿态估计方法研究}

\maketitle

\ContentDes{（一）立项依据与研究内容（建议8000字以下）：}


\NsfcSection{1}{项目的立项依据}{
（研究意义、国内外研究现状及发展动态分析，需结合科学研究发展趋势来论述科学意义；或结合国民经济和社会发展中迫切需要解决的关键科技问题来论述其应用前景。
附主要参考文献目录）；}

\subsection{研究意义}

爆炸品、有毒化工、放射性物质、生物化学制剂等危化品的装配工艺，对于制备环境要求很苛刻。工厂装配过程中，为了确保装配安全性，对产线工人的年龄、工作时间和操作流程等都有严格的要求。以工程炸药制备公司为例，为了保障安全生产，工人正常上工前需培训6个月，每天工作时间不能超过6小时，每项工艺流程需双人确认。这些安全措施极大限制了工厂的产能，亟需通过智能制造技术替代人工，提升产能。

% 此处表述6D姿态估计是智能装配中的关键技术，为了提升精度，需要用多传感器融合
待装配物件的六自由度（6D）姿态估计是智能装配的关键技术，其核心问题是高精度的姿态估计。与普通的工业自动化设备不同，危化品的装配操作流程裕度很小，无法通过降低机械操控设备的控制精度和重访精度实现无人操作。智能装配工艺通过装置在产线上的传感器，估计炸药、放射性元素、腐蚀性药品等空腔的姿态，在操控链路形成负反馈，精准完成原料装填和定位螺丝的紧固。

% 传统的6D姿态估计采用RGB或RGBD的方式计算姿态，准确度以目标物体点云的10%作为度量，无法满足精准装配的需要，亟需改进；当前的学术研究中，6D姿态估计算法估计的准确度以目标物体点云尺度的10\%作为度量，其精度只适用于普通物件如快递包裹、日常物品等的粗抓取，无法满足危化品装配的精细操控要求。
面向危化品智能装配的6D姿态估计需要解决三个核心问题，其一为无纹理表面的算法适应性问题，其二为有遮挡情况下的鲁棒估计问题，其三为姿态估计的高精度修正问题。本项目融合双目相机和光谱相机的影像特征，通过分析多模态传感器数据的特征表达，提取和对齐不同模态的特征，并通过交叉验证抑制单一模态中的噪声，融合有效的互补信息，提升6D姿态估计的适应性、鲁棒性和精度，解决危化品智能装配中的高精度姿态估计难题。

\subsection{国内外相关工作}

大多数姿态估计方法遵循两阶段的范式，首先检测图像中的目标物体，之后在缩放后的目标物体区域上进行姿态估计。尽管现有方法在大多数简单的场景下表现良好，但由于所使用的检测器对无纹理或被遮挡工件的检测效果并不理想，算法在智能装配应用中的性能急剧下降。

对于第一阶段的目标检测任务，常用的检测方法有两段式和单段式两类\cite{ATSS, fcosv1, fcosv2, PAA, faster-rcnn, maskrcnn}。两段式检测方法首先采用区域建议网络~\cite{faster-rcnn, maskrcnn}生成边界框候选体，然后由分类和细化网络处理，去除假阳性，并调整边界框的位置和大小。这种策略准确度较高，但成本高，效率低。单段式检测器通过在编码器的最终特征图中的每个空间位置用一组预定义的锚框代替区域建议网络来解决这个问题~\cite{retinanet,fcosv1,yolov1}。这种方法会导致锚点中存在大量负样本，虽然这一问题可以通过focal loss~\cite{retinanet,fpn}在一定程度上解决，但程度有限，早期的单段式检测器并没有达到两段式检测器的精度。Zhang~\cite{ATSS}通过一个简单而有效的正样本策略解决了这一问题。最近的大多数检测方法都遵循类似的策略~\cite{fcosv2, PAA, autoassign, OTA, TTF, yolov3}，相关改进算法的准确性比两段式方法更好，同时效率更高。但即便如此，这些方法都假设场景中物体的纹理相对丰富，且具有较少的遮挡，与智能装配的6D物体姿态估计场景中的特性仍有较大差异。

% 无纹理物体位姿估计解决方案：在数据输入源头上解决，即增加depth图像和高光谱图像，实现多源融合。
危化品工件的表面大多没有纹理，这种工况下的目标检测是姿态估计中的一个难点。仅通过RGB图像估计位姿，其估计精度不高，鲁棒性较弱。一方面，深度学习网络很难从单幅RGB图像中提取有效的颜色和几何特征，另一方面，三维物体到二维像平面的投影过程丢失了三维结构的几何约束信息，单幅图像无法逆推结构信息。
近几年，基于RGB-D的6D位姿估计方法获得了比单幅RGB图像更高的精度。这类方法充分利用Depth图像生成的点云三维几何表示的能力来预测目标的姿态。算法的核心难点在于RGB和Depth两种异构数据的融和。目前解决的思路有三类。
第一类为像素级融合方法，首先利用2D检测或分割网络提取从RGB图像特征，将图像特征传递给Depth生成的点云，然后将增强后的点云反馈给点云3D目标检测器。2D检测的结果可以辅助三维点云形成3D视锥\cite{Qi2018}，减小候选区域的范围。所形成的的视锥也可以进一步划分为网格单元\cite{Wang2019}，进行3D检测。或者，2D分割的结果也可以用于增强3D点云\cite{Vora2020}，将增强后的点云送入3D目标检测器提升检测效果。这类方法以顺序的方式进行融合，效率较低。
第二类为特征级融合方法，在基于点云的3D目标检测器的中间阶段融合图像和点云特征。例如，在基于网格的检测网络骨干的中间层中使用连续卷积\cite{Liang2018, Liang2019}、混合体素特征编码\cite{Sindagi2019}或Transformer\cite{Zhang2022}网络等融合算子进行多模态融合。
第三类为决策级融合，将RGB图像和Depth图像生成的点云数据通过两个独立网络分别生成2D和3D检测框\cite{Asvadi2018}并融合输出。这种方法可以更好地借鉴每个独立任务的SOTA网络，避免中间特征层上的信息交互，执行效率高，但无法利用不同模式的互补语义信息\cite{Pang2020}提升检测精度。

%本项目在特征级融合的基础上，引入UV数据保持深度图点特征三维空间位置的一致性，从而在异构输入源之间解决视角对齐的问题。
与普通工业零件不同，危化品工件中通常有危险度较高的化学部件，如易爆、有毒或有辐射的填充物。对于这类区域的精细检测和分割，一方面可以避免误操作减少事故，另一方面可为检测网络提供更精准的边界区域特征。不同的化学元素在特定的光谱谱段有脉冲响应特性，可通过光谱相机清晰定位物质边界。对于光谱图像的目标检测和分割，算法思路与可见光图像相似。最主要的问题在于样本数据较少，样本分布不均匀，业界没有大规模数据集可用于网络训练。这一问题可通过小样本\cite{lys2022targetDetection, shi2020HyperspectralTargetDetection}、注意力机制\cite{shi2020hyperspectralROI}等方法解决。

% %此处描述 RGB, Depth, 和光谱数据融合的国内外现状
对于光谱图像与RGB图、深度图的融合，目前没有可借鉴的文献，但类似的多传感器融合方法在遥感影像处理和自动驾驶领域中都有相关研究。......


% 有遮挡情况下的鲁棒估计现状
遮挡是6D姿态估计中的另一个难点问题，通用的目标检测方法假设物体之间遮挡较少，标准真值包围框中心的区域周围为目标物体，因此在网络学习中专注于仅从这些区域提取的样本预测边界框参数。然而，在有遮挡的情况下，真值包围框的中心通常被其他物体或者场景元素遮蔽，导致检测框出现较大偏差。为了提升姿态估计的性能，大多数方法需要依赖额外的姿态修正组件。这些组件首先根据初始姿态以及物体的CAD模型渲染合成图像，然后基于光流网络估计渲染图像和输入目标图像之间的密集2D-to-2D对应关系。在利用目标的3D形状信息将2D对应关系提升到3D-to-2D对应关系之后，基于PnP算法迭代计算更精细的姿态参数。

这种算法框架在大多数通用场景下表现良好，但它有几个缺点。
首先，所使用的光流网络建立在两个假设之上，即两个潜在匹配之间的亮度一致性和本地邻居内匹配的平滑度。这一假设在通用场景下是成立的，但在智能装配的6D物体姿态估计场景下，我们没有关于目标形状的任何线索，缺少形状约束，从而导致目标图像中每个像素的潜在匹配空间盲目扩大。
其次，匹配过程中物体形状信息的缺失会导致匹配结果出现明显偏差，在PnP姿态求解过程中引入显著噪声。
另外，在这种两阶段的框架中，第一阶段的训练依赖于匹配网络的损失函数，该匹配损失不能直接反映最终的6D姿态估计损失，且不是端到端可训练的。

近几年的位姿估计方法通过使网络预测一些预定义的3D关键点~\cite{rad2017bb8, hu2019segDriven, peng2019pvnet, Hu2021}或对每个2D像素预测稠密的对应3D点~\cite{zakharov2019dpod, Su2022, li2019cdpn, wang2021gdrnet, Di2021}来创建对应关系。然后，通过数值PnP求解器~\cite{lepetit2009epnp}或直接从中间对应关系的表达来学习姿态~\cite{hu2020singleStage, EroPnP,wang2021gdrnet, Di2021}。通过精心设计卷积神经网络的结构，相关改进算法~\cite{he2016resnet, resnext_2017_cvpr}在鲁棒性和准确性方面都有显著提高~\cite{Xiang2018, peng2019pvnet, wang2019densefusion60}，但复杂的杂乱场景下，姿态估计的准确性依然不高。

对于6D姿态的修正，以往的方法主要依赖于已配准的深度图~\cite{Xiang2018, li2019cdpn, wang2019densefusion60}，但在许多真实场景~\cite{Hu2021}中深度图是无法获得的。
近几年的姿态修正方法使用无需深度数据的“渲染-比较”策略，可以获得性能相当或更好的结果~\cite{li2018deepim, zakharov2019dpod, cosypose, rad2017bb8, Hu2022, Lipson2022, RNNPose_2022_cvpr,Repose_2021_iccv}。与这一策略不同，Hu等~\cite{Hu2022}最近提出的提出的6D姿态修正方法，将6D姿态修正问题建模为渲染图像到目标图像之间的2D匹配关系，通过数值求解获得修正之后的姿态。该方法相比于“渲染-比较”策略达到了更高的精度，但由于该方法将6D姿态修正建模为无约束的纯2D到2D匹配问题，脱离了6D物体姿态的物理意义，因此理论上讲，该估计结果是次优的。为了提升性能，申请人团队提出了一个由目标的3D形状引导的形状约束递归匹配框架，在精度和效率方面都有了大幅提升。

% 为了解决当前检测器在遮挡环境下的退化问题，我们提出了一种检测方法，该方法利用了6D姿态估计中目标对象是刚性的这一特性。
% 对于此类对象，任何可见部分都可以提供完整边界框的可靠估计。因此，我们认为，与标准物体检测器使用的基于中心的采样相比，任何从可见部分提取的特征都应该是训练期间正样本的潜在候选。

\iffalse
\subsection{国内外相关工作 - 已注释删除}

% \note{刘志强， 这部分要重写，先不要讲问题，先讲国内外研究现状，然后，按照下面的注释重新整理国内外工作。}

智能机器人在工业制造、仓储和物流领域已得到广泛应用，相对成熟的技术包括目标检测、识别、跟踪和图像分割等。为了进一步提升机器人的操作技能，目标物体的6D姿态估计在近几年受到更多关注。6D姿态估计中的平移参数相对较为简单，主要难点在于旋转参数的估计，大量的学术研究着重于解决这一问题。

一般的姿态估计应用系统采用可见光相机捕获的目标物体的RGB图像作为输入。最直接的思路是将姿态估计作为一个回归或分类任务，直接从输入图像中预测物体的姿态参数，如欧拉角或旋转四元数。基于回归思路的代表算法是向宇等人提出的端到端深度学习模型PoseCNN\cite{Xiang2018}。该网络采用Encoder-Decoder结构，在Encoder部分提取到深层特征后，基于结构先验，通过两个独立的Decoder分支分别估计平移和旋转参数。对于平移参数估计分支，网络首先定位图像中的2D目标中心和深度，然后根据投影变换矩阵计算平移参数。对于旋转参数估计分支，采用全连接网络FCN回归旋转四元数。与向宇的思路不同，Kehl等人用分类的思路进行姿态估计，他将二维图像目标检测网络SSD\cite{liu2016ssd}结构扩展到6D位姿估计中，将旋转空间离散化为可分类的视点，提出了SSD-6D\cite{kehl2017ssd}网络。Trabelsi等人将SSD-6D中的目标检测部分用Yolo替换，并加入注意力机制和姿态优化模块，提出了位姿建议网络PPN\cite{trabelsi2021pose}，显著提升了姿态估计的精度。上述算法的网络设计都存在一个问题，即需要通过迭代计算优化最终输出的位姿，这一过程相当耗时。Bukschat在SSD-6D思路基础上，加入了新的数据增强策略，并将目标检测网络替换为EfficientDET\cite{tan2020efficientdet}，提出了EfficientPose\cite{bukschat2020efficientpose}网络，使姿态估计精度得到了进一步提升。EfficientPose最大的优势是剔除了低效的后期优化部分，通过前端数据增强策略提升整体估计精度。

深度学习算法能够通过复杂的网络结构拟合自然图像到6D姿态的映射函数，但这一问题本身是病态问题，通过直接回归或分类的位姿估计模型在泛化性上有明显的缺陷。相对而言，将3D模型和2D投影图像之间的对应关系加入网络设计，更有利于提升算法的泛化性能。Rad等人\cite{rad2017bb8}在分割网络之后加入了专门的预测分支，用于预测物体三维包围框（Bounding Box）角点的二维投影点，并基于这种2D-3D点的对应关系，通过求解PNP问题估计6D姿态参数。所提出的BB8网络\cite{rad2017bb8}虽然思路新颖，但存在诸多问题，算法精度不高，无法端到端训练，且前向推理耗时长。Tekin将二维目标检测的YOLO\cite{redmon2016yolo}框架扩展为3D检测结构，替换BB8中的目标检测模块，提出了YOLO-6D网络。除PNP部分外，YOLO-6D的其他部分可以实现端到端训练。相对BB8网络，推理时间缩短了94.1\%，精度提升了28.3\%。这种两阶段网络架构思路被后续许多工作所采用，Oberweger等人\cite{oberweger2018}提出以二维热图 (2D heatmaps) 的形式预测三维关键点的二维投影；Peng等人\cite{peng2019pvnet}提出的PVNet采用基于投票的关键点定位策略；Yu等人\cite{yu20206dof}改进了PVNet的损失函数，将像素和关键点之间的距离纳入目标；Song等人在关键点中间表示的基础上，加入了边向量和对称对应特征，提出了HybridPose\cite{song2020hybridpose}网络，这些网络结构通过不同二维关键点定位的改进，提升了模型在遮挡和截断场景下的性能。

上述网络仅对物体表面稀疏的关键点提取2D-3D映射关系，在有遮挡和噪声的情况下精度较低。Li对2D图像目标区域的每一个像素预测其3D对应点坐标，建立2D-3D的稠密映射关系，集成到所设计的CDPN\cite{li2019cdpn}姿态估计网络中，在遮挡和有噪声情况下相对HybridPose网络精度提升了13.4\%。在此基础上，Park等人提出的Pix2Pose\cite{park2019pix2pose}在CDPN的损失函数中增加了坐标变换损失项，Zakharov等人提出的DPOD\cite{zakharov2019dpod}通过分别回归U、V两个坐标值构建的UV图建立2D-3D的对应关系，Hodan等人提出的EPOS\cite{hodan2020epos}采用紧凑的表面碎片表示物体从而建立像素与碎片之间的映射关系，这几个改进算法进一步提升了对称物体的姿态估计精度，在一定程度上解决具有全局或部分对称的物体等具有挑战性的情况。

虽然以上算法通过稠密对应关系能够对严重遮挡和物体对称性具有一定的鲁棒性，然而由于位姿估计连续搜索空间较大，回归目标稠密坐标比预测稀疏关键点更困难，实际的位姿估计精度并不够高。
Li等人提出的DeepIM\cite{li2018deepim}，在给定初始估计姿态的情况下，通过最小化当前姿态下观测图像和渲染图像之间的差异来迭代细化初始姿态。在此基础上，Manhardt等人\cite{manhardt2018deep}通过对齐当前姿态下观测图像和渲染图像之间的物体轮廓来驱动姿态更新，Trabelsi等人\cite{trabelsi2021pose}将注意力机制整合到深度神经网络中突出目标重要部位从而细化初始位姿，Yen-Chen等人\cite{yen2021inerf}提出的iNERF优化神经辐射场框架将渲染的更加真实的图像与当前位姿下观测图像进行对比优化初始位姿。以上优化算法作为6D位姿估计网络的二次优化集成到一些实例级的物体位姿检测模型中，以产生更准确的结果。例如，PoseCNN\cite{Xiang2018}获取初始位姿，然后结合DeepIM\cite{li2018deepim}迭代优化初始位姿估计结果，最终取得良好的效果。

基于迭代优化的二阶段位姿估计模型，其速度在很大程度上依赖于迭代次数和所使用的渲染器，这成为其推广使用的瓶颈。且以上网络均仅以目标物体的RGB图像作为输入，其位姿估计精度有限。一方面，RGB图像在缺乏纹理、光照不足、过度曝光的场景下，难以通过深度学习网络提取目标颜色特征和几何特征；另一方面，三维物体投影到二维像平面的过程，部分丢失了三维物体本身的三维结构几何约束信息。随着双目RGB相机的发展，基于RGB-D的六自由度位姿估计方法成为目前位姿估计的主流方式。基于RGB-D的方法以RGB和Depth图像作为输入，充分利用点云三维几何表示的能力来预测目标的姿态。

通过RGB-D预测物体姿态的一个直接解决方案是遍历当前物体姿态所有可能性。首先，我们可以利用CAD模型生成一个包含所有可能的物体姿态的RGB-D数据集。然后，在给定RGB-D图像或深度掩模 (depth mask) 的情况下，从数据集中检索与目标物体最相似的图像来确定目标物体的姿态。检索可以通过比较图像描述子(image descriptors) 或匹配模板来实现。本课题将这类方法称为基于检索的方法。

传统方法\cite{hinterstoisser43,huttenlocher44,steger45,hinterstoisser46,hinterstoisser47}利用手工设计的特征实现检索或模板匹配，因此感知能力有限。受传统的基于检索方法的启发，据我们调研所知，Kehl等人\cite{kehl48}是第一个提出使用RGB-D从头开始训练卷积自动编码 (CAE, Convolutional Auto Encoder)\cite{masci49}来学习描述子用于检索。虽然与传统方法相比有了很大的改进，但Kehl等人的工作在处理遮挡和无纹理对象方面面临挑战。因此，Park等人\cite{park50}提出了多任务模板匹配算法 (MTTM, Multi-Task Template Matching)。MTTM只使用深度掩膜作为输入，以找到与目标对象最接近的模板。在MTTM网络架构中，同时进行目标分割和姿态变换以去除不相关点。通过消除无关点，MTTM在位姿估计中表现出较强的鲁棒性。但是，如果不使用CAD模型，MTTM的性能会大大下降。与上述直接依赖于网络体系结构本身学习描述符或模板的方法相比，一些方法在其网络结构中中引入了三元组比较（triplet comparison），以学习更有效的描述子。例如，Wohlhart等人\cite{wohlhart51}提出了一种基于深度学习的描述子预测方法，该方法使用三元损失 (triplet loss)，成功地将不同的对象和视图分解成簇。基于这一工作\cite{wohlhart51}，Balntas等人\cite{balntas52}进一步发展了一种新的描述子表示法，它在所学特征和位姿标签差异之间建立直接关系，以更好地表示隐式空间中的物体位姿。之后，Zakharov等人\cite{zakharov53}构造角度差函数，并设置一个恒定的角度阈值作为三元损失的动态裕度，以获得更快的训练速度和更好的性能。

总之，基于检索的方法在大多数情况下都能获得稳健的性能。然而，它们需要对旋转空间进行离散来定义编码空间 (codebook)，当离散间隔较大时会造成粗糙的预测结果，当离散间隔较小时会导致计算效率低下。

与基于RGB的方法类似，在基于RGB-D的方法中，也有一系列的工作通过提取对象的关键点来预测对象的姿态。这类方法通常是利用预测的关键点构造三维到三维 (3D-3D) 的对应关系来求解目标位姿。据我们调研所知，PVN3D\cite{he2020pvn3d54}是第一个基于深度学习通过提取3D关键点成功估计物体位姿工作。PVN3D设计了一个深度霍夫投票网络 (deep Hough voting network)来检测物体的三维关键点，然后以最小二乘拟合的方式估计物体的6D位姿参数。PointPoseNet\cite{chen2020pointposenet55}结合颜色和几何信息的优点对PVN3D进行了改进。具体来说，PointPoseNet同时预测分割掩码并回归指向3D关键点的逐点单位向量。然后利用分割点的单位向量，借助几何约束条件，生成最佳姿态假设。PVN3D和PointPoseNet都选择使用通过最远点采样选择关键点（FPS, Farthest Point Sampling）。与之相比，FFB6D (用于6D位姿估计的全流双向融合网络)\cite{he2021ffb6d56}提出了一种新的关键点选择算法SIFT-FPS，通过充分利用纹理和几何信息以取代FPS。SIFT-FPS主要包含两个步骤。第一步提取SIFT特征提取二维关键点。第二步，将2D关键点转换为3D关键点，并通过FPS对关键点进行选择。通过这种方式选取的关键点在物体表面的分布更均匀，纹理更清晰。最近，Wu等人\cite{wu2022vote57}认为以前的矢量或偏移方案对离散关键点敏感，因此提出RCV-POSE模型。在RCV-Pose中，引入了径向投票方案，对于每个目标点，使用一个深度学习模型来学习多个球面。每个关键点都可以位于球体的表面上。因此，在推理过程中，该算法只需利用这些球面的表面来投票选择一个三维累加器空间（3D accumulator space），累加器空间的峰值标示为关键点的位置。与以往的方法相比，RCV-Pose只需预测3个关键点，效率明显提高。

除上述方法外，许多工作以设计模型满足某些特殊的要求，而不是仅仅为了提高精度。首先，一些方法试图解决由遮挡和杂乱等复杂场景引起的问题。例如，Alexander等人\cite{krull2015learning58}提出了一种学习比较观察到的RGB-D图像和渲染图像的方法，以更好地处理遮挡和复杂的传感器噪声，该方法通过用比较观察到的图像和渲染图像的卷积神经网络(CNN)描述特定物体姿态的后验密度来实现。Jafari等人\cite{hosseini2019ipose59}提出了一种基于实例分割和稠密坐标回归的三步分解方法来处理遮挡，该方法被称为第一个基于深度学习的局部遮挡物体精确姿态估计器。然后，Wang等人\cite{wang2019densefusion60}提出了DenseFusion，它充分利用了颜色和深度信息，从而在严重遮挡和复杂光照条件下具有鲁棒性。此外，Morefusion\cite{wada2020morefusion61}结合周围空间感知进行位姿预测，并联合多目标位姿优化，在遮挡严重、目标较小的杂乱场景中极大地提高了位姿估计的一致性和准确性。近期，基于CNN的6D姿态修正方法获得了越来越多学者的关注，该类方法的目的是使用一个单独的网络来修正姿态估计网络估计的不精确的6D姿态。由于其迭代优化的范式，该类方法可以在性能和精度方面进行权衡。胡等人\cite{Hu2022}提出的PFA网络证明基于光流的6D姿态修正方法具有优良的领域泛化能力，即在合成图像上进行训练，即可泛化到真实图像上。Lipson等人\cite{Lipson2022}进一步结合多视图与可微分捆绑调整层来使姿态修正网络可以利用深度图像来训练与测试。本项目拟在此基础上，提出一个由目标三维形状引导的形状约束递归匹配的6D位姿估计网络。


% ---2023-2-10 17:34:26

% 从这开始引入光谱，介绍光谱图像的特点，论证增加光谱数据为什么能提高精度的机理
尽管RGB图像和Depth图像能够提供颜色特征和形状特征，但是针对危化品智能装配领域而言，操作的精细度和鲁棒性要求更高。另一方面，由于危化品装配过程中不同种类零部件具有相似的外观特征（包括颜色特征和几何特征），仅通过RGB和Depth图像难以细致区分。为了捕获危化品装配部件更多细节特征，本项目拟引入高光谱相机拍摄的高光谱图像，配合双目相机弥补RGB和Depth图像对危化品装配部件定位定姿能力的不足。

% 危化品因其独特的成分，会对电磁波谱产生独特的反应。串联光谱的相关工作
高光谱成像能够获取物体在各个波段内电磁波发射或者反射的光亮度值，所得到的更为精细且具有特异性的光谱数据为目标识别与定位提供了巨大的优势。传统的高光谱图像目标检测受空间分辨率与成像速度的限制，其实时性较差且十分依赖目标的光谱特征。目前，基于深度学习的目标检测算法迅速发展，这为实时高光谱图像目标检测提供了新思路，使高光谱图像目标检测同时利用空间与光谱信息完成对目标的精准定位与识别成为了可能。由于光照、传感器噪声的干扰，高光谱目标检测面临着类内不相似和类间相似的重大挑战。为了有效抑制背景和增强目标,Li等人\cite{lys2022targetDetection}提出了一种基于无约束线性混合模型，通过分层去噪自动编码器减少干扰，然后通过子空间投影进行精确目标检测。为了解决训练样本有限和不同传感器获取高光谱图像噪声分布不一致的问题，Shi等人\cite{shi2020HyperspectralTargetDetection}提出了一种新的半监督域自适应少样本学习模型SDAFL,采用残差通道注意力机制和加权域适应模块自动选择具有代表性的特征，然后利用先验已知目标特征的区别性提升损失函数进一步增强特征区分，提高类内相似度，抑制类间相似度。为了解决空间维和光谱维冗余以及光谱变化的问题，Shi等人\cite{shi2020hyperspectralROI}提出的DSSN网络架构采用感兴趣区域特征变换模块减少空间冗余，同时利用多尺度光谱注意力机制去除光谱冗余。

% 跟研究内容结合起来，研究内容有三，融合，3D检测，和几何引导。
虽然高光谱数据的引入理论上能够为目标识别和定位提供巨大优势，但是针对复杂环境下危化品装配零部件姿态估计任务而言，当前算法存在以下问题：RGB，depth和光谱数据同时作为输入存在多模态信息对齐和融合的问题。在信息对齐上，不同信息的表达方式不同，需要从特性中找共性，才能在同一个系统中将二维图像、三维模型以及深度点云信息对齐。此外，因为数据样本中存在噪声，在噪声的干扰下，很显然是没有办法做到精准对齐的。在信息融合上，不同模态信息需要不同的处理方式，如何高效结合多个特征信息，也是需要研究的问题之一。

针对多源融合表征的问题，依据融合位置的不同可以分为三类（以基于RGB和Lidar的数据融合为例）：第一类，前融合方法。该方法首先利用2D检测或分割网络从图像中提取知识，然后将图像知识传递给点云，最后将增强后的点云反馈给基于激光雷达的点云3D目标检测器。例如，文献\cite{Qi2018}提出了一种区域级融合方法生成3D视锥，并将其应用于激光雷达点云来减小目标候选区域的范围。华南理工大学的贾奎课题组在此基础上将3D视锥划分为网格单元，并在网格单元上应用卷积网络进行3D检测\cite{Wang2019}。文献\cite{Vora2020}利用基于图像的语义分割来增强输入点云，将增强后的点云送入激光雷达检测器，获得了较好的检测结果。除了语义分割，也有一些研究利用图像中的其他信息，例如使用深度图像来增强稀疏的3D点云\cite{Yin2021}。尽管前融合方法通过有效的预处理提高了检测性能，但是这种以顺序的方式执行多模态融合和3D目标检测的过程带来了不可忽视的时间成本；第二类，中融合方法。该方法在基于激光雷达的3D目标检测器的中间阶段融合图像和激光雷达特征。例如，在基于网格的检测网络骨干的中间层中，使用连续卷积(Continuous convolutions)\cite{Liang2018,Liang2019}、混合体素特征编码(Hybrid voxel feature encoding)\cite{Sindagi2019}和变换(Transformer)\cite{Zhang2022}网络等融合算子进行多模态融合。文献\cite{Chen2017}提出了一种多视角融合方法，在提议生成(Proposal generation)阶段进行多通道特征融合。这类方法建议对多模态表示进行更深层的融合，并产生更高质量的3D框，但在由于相机和激光雷达的特征本质上是异构的，因此在融合机制和视角对齐方面仍存在问题。第三类，后融合方法。该方法将图像和激光雷达数据分别进行单独处理，并将输出的2D和3D框融合\cite{Asvadi2018}。与前两类融合方法相比，该方法不仅可以更好的利用每个通道现有的网络，而且还避免了中间特征或输入点云上的复杂交互，拥有较高的执行效率。但是，由于输出结果不依赖于图像和激光雷达传感器的深度特征，这种方法无法利用不同模式的丰富语义信息，直到近几年才受到研究者关注\cite{Pang2020}。本课题将引入高光谱图像融入到RGB和D图像输入中，并研究多源融合表征的问题，实现多模态数据的对齐和融合，为后续任务提炼有效信息。

在实现多模态数据对齐和融合后，需要使用2D目标检测器在危化品场景中提取出装配零部件准确的2D边界框\cite{Di2021,Cai2022,Haugaard2022}，获得感兴趣区域(Region of Interest, RoI)。现有的2D目标检测主要遵循两种策略：二阶段检测和一阶段检测。二阶段检测器首先使用区域建议网络(Region Proposal Network, RPN)来生成边界框候选，然后用分类和细化网络处理候选边界框以去除假正(False Positive, FP)样本并调整边界框的位置和大小\cite{He2020,Ren2017}。这种策略具有较高的检测精度，但是在实际危化品智能检测应用中需要较高的成本并且计算效率较低。一阶段检测器在编码器最后的特征图中的每个空间位置用一组预定义的锚点(Anchor)代替区域建议网络\cite{Redmon2016}，取得了较高的计算效率。但是，预定义的锚点中存在大量的负(Negative)样本导致检测性能较差。文献\cite{Lin2017}通过焦点损失(Focal Loss)在一定程度上解决了样本数量不平衡问题，但是仍然没有达到二阶段检测器的精度。文献\cite{Zhang2020}在一阶段检测器中对正候选样本进行采样，有效地解决了负样本过多的问题。许多最新的检测方法遵循类似的策略\cite{Tian2022,Ge2021}，实现了比二阶段检测方法更加准确和高效的目标检测。这些方法在标准的目标检测基准测试中表现出了很好的性能，但是它们在6D位姿估计的基准测试中通常会受到严重遮挡等复杂因素的影响出现检测缺失或不准确的情况。本项目拟在此基础上\cite{Tian2022,Ge2021}，利用刚性目标对象的可见部分来获取目标边框的可靠估计，为最终6D姿态估计提供鲁棒的目标检测输入。




% 6D姿态估计算法的研究主要有两大类，一类针对RGB图像\cite{Marullo2022}，另一类针对RGBD图像\cite{Sahin2019}，对于不同类型的输入图像，其方法差异性比较大\cite{ZhangShaoBoPhd}。

% \note{刘志强，关于相关工作现状的介绍，可以参考\cite{ZhangShaoBoPhd}}


% 先讲基于RGB的方法，通过边介绍边引用的方式，分析每篇论文解决的问题，将参考文献串联起来。将我们的已有工作，包括胡老师的工作都放进来。

% 再讲基于RGBD的方法，通过边介绍边引用的方式，分析每篇论文解决的问题，将参考文献串联起来。将我们的工作，包括点云分割（郝丰达）、点云补全（夏亚奇）的工作都放进来。

% 1. 分析基于RGB或RGBD的方法的缺陷。
%% A. 精度太低，10%就认为是正确的。
%% B. 工业品中存在大量的无纹理工件，且工件与自动化生产设备的纹理相同，本身会造成精度不高。
%% C. 对先验信息的利用不充足，我们可以通过光谱图像分析炸药轮廓，根据炸药装填位置分析空腔的轮廓，进而提升位姿估计的精度。
% 2. 引出我们准备做的双目RGB+光谱传感器的思路。分析光谱数据的特点，将我们在光谱重建、分类、检测、分割等方面的已有工作在这里都串联起来。
% 3. 分析为什么双目RGB和光谱传感器能提升精度。

% \subsubsection{基于RGB图像的物体六自由度姿态估计方法}

% 随着深度学习技术的发展，越来越多的基于深度学习的物体六自由度方法被提出。估计物体六自由度位姿最直接的方法就是让深度学习模型直接通过RGB图像预测位姿相关参数。一般来说，我们将基于深度学习的方法分为五大类：直接方法、基于关键点的方法、基于稠密坐标的方法、基于细化的方法和自监督的方法。

% 1) 直接法

% 预测六自由度姿态最直接的方法之一是将物体姿态估计作为一个回归或分类任务，直接从输入图像中预测与物体姿态相关的参数表示 (例如欧拉角或旋转的四元数)。例如，向宇等人\cite{Xiang2018}提出了一种用于端到端六自由度姿态估计的深度学习模型PoseCNN。它将目标姿态估计任务解耦为两个部分：一方面，对于平移估计，PoseCNN首先定位图像中的2D目标中心，然后估计距离摄像机的深度再根据投影方程恢复三维平移；另一方面，对于三维旋转，通过回归一个四元数表示来估计。这两个部分都利用了几何先验来简化任务。SSD-6D\cite{kehl2017ssd}是另一种直接预测位姿的网络架构。它将二维图像目标检测SSD\cite{liu2016ssd}网络结构扩展到六自由度位姿估计。该方法将旋转空间离散化为可分类的视点，将旋转估计作为一个分类问题来处理，而不是连续地回归旋转参数。最近，Trabelsi等人\cite{trabelsi2021pose}提出了一种用于六自由度物体位姿测量的位姿建议网络（pose proposal network）。该位姿建议网络提出了一种基于CNN的编码器-多解码器模块。多解码器将旋转估计、平移估计和置信度估计解耦到不同的解码器中。因此，每个子任务可以学习到特定的特征。

% 上述三种方法虽然直观，但都高度依赖于耗时的姿态细化操作（pose refinement operation）来提高性能。与它们形成对比的是，EfficientPose\cite{bukschat2020efficientpose}将EfficientDET\cite{tan2020efficientdet}结构引入到直接姿态估计框架。通过使用新颖的6D增强策略，EfficientPose在无需细化操作的情况下获得了优越的性能。然而，由于通过RGB预测物体六自由度位姿是一个不适定问题（ill-posed problem），直接方法往往表现不佳或不能很好地推广到自然场景。

% 目前，更好和更流行的选择是建立2D-3D对应关系，并求解它们来预测姿态相关参数。最近，另一个分支\cite{hu2020singleStage,chen2020end2end,wang2021gdrnet}试图在直接预测方法中采用这种思想。更具体地说，他们试图将间接方法改进为直接方法，利用神经网络直接建立2D-3D对应关系，并利用深度学习网络模拟Perspective-N-Point (PNP)\cite{lepetit2009epnp}算法。这样，将对应关系提取网络与模拟PNP网络相结合，就可以直接回归出目标的姿态。例如，GDR-Net\cite{wang2021gdrnet}首先使用ResNet\cite{he2016resnet}主干网络回归几何特征来构建2D-3D对应关系。然后，将2D-3D对应关系输入到由CNNs和全连接层 (fully connected layers) 组成的Patch-PnP求解器中，以预测目标位姿。GDR-Net的整个框架可以进行端到端的训练。虽然需要2D-3D对应，但可以通过网络直接预测物体的姿态。

% 2）关键点法

% 如前所述，与直接预测姿态相关参数相比，建立2D-3D对应关系进行目标姿态检测更为准确。在现有的基于对应关系的方法 (correspondence-based methods) 中，基于关键点的方法利用CNNs检测图像中的2D关键点，然后解决PNP问题\cite{lepetit2009epnp}。据我们调研所知，BB8\cite{rad2017bb8}是第一个基于深度学习建立2D-3D对应关系进行目标姿态检测的算法。BB8首先使用一个深度神经网络来粗分割物体，然后使用另一个网络来预测三维包围盒角点的二维投影，从而建立2D-3D对应关系。基于这种2D-3D对应关系，通过求解PNP问题可以估计出六自由度的位姿。

% 由于BB8是一个多阶段的网络结构，因此BB8不能进行端到端的训练且推理耗时长。与BB8类似，Tekin等人\cite{tekin2018realTime}采用YOLO\cite{redmon2016yolo}框架用于二维检测的思想，并将其扩展用于六自由度目标位姿检测，即Yolo-6D。 Yolo-6D的关键部件是一个单阶段 (single-shot) 目标检测深度神经网络。它以RGB图像为输入，直接检测三维包围盒角点的二维投影，然后用PNP求解器估计目标位姿。这种两阶段的网络架构产生了一个快速和准确的六自由度位姿预测，不需要任何后处理。

% 之后，为了解决遮挡和截断复杂环境下的六自由度位姿估计的挑战，这种两阶段网络架构方法被许多工作\cite{pavlakos2017,zhao2018estimating,oberweger2018,hu2019segDriven}所采用。例如，Oberweger等人\cite{oberweger2018}提出以二维热图 (2D heatmaps) 的形式预测三维关键点的二维投影。与直接回归二维关键点坐标相比，从热图中定位关键点在一定程度上解决了遮挡问题。但是，由于热图的大小是固定的，因此很难处理截断的对象，因为它们的一些关键点可能在图像之外。为了解决这个问题，PVNet\cite{peng2019pvnet}采用了基于投票的关键点定位策略。具体来说，它们首先训练一个CNN来回归指向关键点的像素向量，即向量场，然后使用属于目标对象像素的向量来投票选择关键点位置。由于这种矢量场表示，被遮挡或截断的关键点可以从可见部分中稳健地恢复。因此，PVNet在严重遮挡或截断的情况下仍能获得良好的性能。受PVNet的启发，一些研究工作还执行了一种基于像素的关键点定位投票方案以提高性能。例如，于等人\cite{yu20206dof}在PVNet的基础上提出了一种有效的损失，通过将像素和关键点之间的距离纳入目标，实现更精确的矢量场估计。然后，HybridPose\cite{song2020hybridpose}将单一中间表示推广到包含关键点、边向量和对称对应的混合中间表示。这种混合表示利用不同的特征来实现精确的姿态预测。然而，回归更多的表示也会造成更多的计算代价，限制了推理速度。

% 3）稠密坐标法

% 除了关键点之外，另一种基于对应的方法是基于稠密坐标的方法，它将六自由度目标位姿估计任务描述为建立稠密的2D-3D对应关系，然后用PNP求解器恢复目标位姿。通过预测每个目标像素的三维目标坐标或预测密集的UV图来获得密集的2D-3D对应关系。在深度学习方法流行之前，早期的工作通常使用随机森林来预测对象坐标\cite{brachmann2014,krull2015,nigam2018}。然后Brachmann等人\cite{brachmann2016uncertainty}将标准随机森林扩展为自动上下文回归框架，迭代回归减少预测对象坐标的不确定性。然而，由于随机森林只能在简单场景中学习有限的特征，所以这些早期的研究工作性能较差。 为了处理更复杂的情况，最近的研究引入了深度学习网络来预测物体像素的三维坐标。据我们调研所知，


% CDPN\cite{li2019cdpn}是第一个采用深度学习模型进行稠密坐标预测的方法。在CDPN中，利用预测的三维目标坐标建立二维图像与三维模型之间的紧密对应关系，对遮挡和杂波具有较强的鲁棒性。然而，CDPN忽略了处理对称性的重要性。为了准确估计对称物体的六自由度位姿，Pix2Pose\cite{park2019pix2pose}建立了一个新的损失函数，即变换损失 (transformer loss)，它可以将每个像素的三维坐标变换到其最接近的对称位姿三维点上。

% 上述方法都是直接回归稠密的三维物体坐标，该类方法在对称物体上无法做到连续预测。DPOD\cite{zakharov2019dpod}不是直接回归3D坐标，而是通过UV图建立2D-3D对应关系。在给定像素颜色的基础上，根据对应图估计其在三维模型表面的对应位置，从而提供图像像素与三维模型顶点之间的关系。

% 无论是预测稠密坐标还是UV图，都是对物体上所有可见点进行预测，效率低，学习困难。而在EPOS\cite{hodan2020epos}中，目标物体由紧凑的表面碎片表示，密集采样的像素与碎片之间的对应关系是使用编码器/解码器网络预测的。这种预测方法可以在一定程度上解决具有全局或部分对称的物体等具有挑战性的情况。

% 总之，基于稠密坐标的方法对严重遮挡和对称性具有一定的鲁棒性。然而，由于连续搜索空间较大，回归目标稠密坐标比预测稀疏关键点更困难。

% 4）细化法

% 在给定初始姿态估计的情况下，DeepIM\cite{li2018deepim}利用基于深度学习的姿态细化网络，通过最小化当前姿态下观测图像 (the observed image) 和渲染图像 (the rendered image) 之间的差异来迭代细化初始姿态。然后将改进后的姿态作为下一次迭代的初始姿态，直到改进后的姿态收敛或迭代次数达到阈值。同样，Manhardt等人[30]还采用了迭代匹配流水线，并提出了一种新的视觉损失，通过对齐物体轮廓来驱动姿态更新。此外，DPOD\cite{zakharov2019dpod}提出了一个独立的姿态细化网络，包括三个独立的输出头用于回归旋转和平移。在Trabelsi等人的工作中\cite{trabelsi2021pose}，注意力机制被整合到深度神经网络中，通过突出目标重要部位细化初始位姿。上述方法侧重于对渲染图像和观测图像的比较，忽略了真实感渲染的重要性。最近，Yen-Chen等人\cite{yen2021inerf}提出了一种优化神经辐射场的框架iNERF，并取得了良好的效果。

% 这些精化网络已经被集成到一些实例级的物体位姿检测模型中，以产生更准确的结果。例如，PoseCNN\cite{Xiang2018}与DeepIM\cite{li2018deepim}相结合。然而，基于求精的方法的速度在很大程度上依赖于迭代次数和所使用的渲染器，这成为其推广使用的瓶颈。 

% 5）自监督法

% 在六自由度目标位姿检测任务中，目前的深度学习模型高度依赖于对真实世界的标注数据进行训练，而这些标注数据很难获得。一个直观的方法是使用成本较低的合成数据进行训练。然而，许多工作已经证明，由于合成数据和自然数据之间存在巨大的领域差距，单纯基于合成数据训练的模型对现实世界场景的泛化能力很差。

% 因此，域随机化(Domain Randomization, DR)\cite{tobin2017domainRandomization}被提出并使用\cite{sundermeyer2018implicit}。其核心思想是通过对物体姿态进行随机采样，并将模型放置在随机背景图像上生成合成的图像。通过这种合成方式，真实世界域只是生成域的一个子集，因此模型就可以利用这些合成图像学习尽可能多的六由度姿态相关特征。此外，许多方法试图生成更真实的渲染\cite{wang2019normalized,movshovitz2016photoRealistic,wen2020se3}或使用数据增强策略\cite{bukschat2020efficientpose,peng2019pvnet}。

% 然而，这些方案都不能解决严重遮挡等问题，在实际数据上的性能也比较差。在此背景下，引入了自监督学习的思想。例如，邓等人\cite{deng2020selfSupervised}提出一种新颖的机器人系统，用精确的六维物体姿态标记真实世界的图像。通过与环境中的对象交互，系统可以生成新的自监督学习数据，并以终身学习的方式改善姿态估计结果。最近，Self6D\cite{wang2020self6d}利用神经辐射场\cite{chen2019nerf}提出了一种自监督的六自由度姿态估计解决方案。Self6D首先以完全监督的方式对合成数据训练网络，然后通过自监督的方式对未标记的真实RGB数据进行微调，寻求真实图像和渲染图像之间的视觉和几何最佳对齐。同样，Sock等人\cite{sock2020}提出了一种两阶段六自由度物体位姿估计器框架。在他们的工作中，第一阶段加强渲染预测和真实输入图像之间的姿态一致性，缩小这两个领域之间的差距。第二阶段通过加强对不同对象视图之间的光度一致性来优化先前训练的模型，其中一个图像对齐以匹配另一个视图，从而使它们能够进行比较。

% 另外，上面介绍的iNERF\cite{yen2021inerf}不需要使用任何实际的注释数据进行训练。 因此，它也属于这一类方法。DSC-PoseNet\cite{yang2021dscPosenet}受基于关键点的方法\cite{peng2019pvnet}和\cite{song2020hybridpose}的启发，提出了一种基于弱监督和自监督学习的姿态估计框架，其中弱监督分割方法和自监督关键点学习方法可以在不使用姿态标注的情况下实现双尺度关键点一致性。 最近，Li等人\cite{li2020hu}利用输入图像与其DR增强对应物之间的一致性来实现自监督学习。结果表明，该自监督框架能够在具有挑战性的条件下鲁棒、准确地估计出六自由度的位姿，并对基于自监督的实例级单目目标位姿检测模型进行了排序。 

% \subsubsection{基于RGB-D图像的物体六自由度姿态估计方法}

% RGB图像缺乏深度信息(Depth, D)，使得六自由度目标位姿检测任务成为一个不适定问题 (ill-pose problem) 。随着双目RGB相机的发展，基于RGB-D的六自由度位姿估计方法成为目前位姿估计的主流方式。基于RGB-D的方法以RGB和Depth图像作为输入，充分利用点云表示的能力来预测目标的姿态。通常，基于RGB-D的方法可以分为基于检索的方法、基于关键点的方法和其他基于深度学习的方法。

% 1）检索法

% 预测物体姿态的一个直接解决方案是遍历当前物体姿态所有可能性。首先，我们可以利用CAD模型生成一个包含所有可能的物体姿态的RGB-D数据集。然后，在给定RGB-D图像或深度掩模 (depth mask) 的情况下，从数据集中检索与目标物体最相似的图像来确定目标物体的姿态。检索可以通过比较图像描述子(image descriptors) 或匹配模板来实现。本课题将这类方法称为基于检索的方法。

% 传统方法\cite{hinterstoisser43,huttenlocher44,steger45,hinterstoisser46,hinterstoisser47}利用手工设计的特征实现检索或模板匹配，因此感知能力有限。受传统的基于检索方法的启发，据我们调研所知，Kehl等人\cite{kehl48}是第一个提出使用RGB-D从头开始训练卷积自动编码 (CAE, Convolutional Auto Encoder)\cite{masci49}来学习描述子用于检索。虽然与传统方法相比有了很大的改进，但Kehl等人的工作在处理遮挡和无纹理对象方面面临挑战。因此，Park等人\cite{park50}提出了多任务模板匹配算法 (MTTM, Multi-Task Template Matching)。MTTM只使用深度掩膜作为输入，以找到与目标对象最接近的模板。在MTTM网络架构中，同时进行目标分割和姿态变换以去除不相关点。通过消除无关点，MTTM在位姿估计中表现出较强的鲁棒性。但是，如果不使用CAD模型，MTTM的性能会大大下降。与上述直接依赖于网络体系结构本身学习描述符或模板的方法相比，一些方法在其网络结构中中引入了三元组比较（triplet comparison），以学习更有效的描述子。例如，Wohlhart等人\cite{wohlhart51}提出了一种基于深度学习的描述子预测方法，该方法使用三元损失 (triplet loss)，成功地将不同的对象和视图分解成簇。基于这一工作\cite{wohlhart51}，Balntas等人\cite{balntas52}进一步发展了一种新的描述子表示法，它在所学特征和位姿标签差异之间建立直接关系，以更好地表示隐式空间中的物体位姿。之后，Zakharov等人\cite{zakharov53}构造角度差函数，并设置一个恒定的角度阈值作为三元损失的动态裕度，以获得更快的训练速度和更好的性能。

% 总之，基于检索的方法在大多数情况下都能获得稳健的性能。然而，它们需要对旋转空间进行离散来定义编码空间 (codebook)，当离散间隔较大时会造成粗糙的预测结果，当离散间隔较小时会导致计算效率低下。

% 2）关键点法

% 与基于RGB的方法类似，在基于RGB-D的方法中，也有一系列的工作通过提取对象的关键点来预测对象的姿态。这类方法通常是利用预测的关键点构造三维到三维 (3D-3D) 的对应关系来求解目标位姿。据我们调研所知，PVN3D\cite{he2020pvn3d54}是第一个基于深度学习通过提取3D关键点成功估计物体位姿工作。PVN3D设计了一个深度霍夫投票网络 (deep Hough voting network)来检测物体的三维关键点，然后以最小二乘拟合的方式估计物体的6D位姿参数。PointPoseNet\cite{chen2020pointposenet55}结合颜色和几何信息的优点对PVN3D进行了改进。具体来说，PointPoseNet同时预测分割掩码并回归指向3D关键点的逐点单位向量。然后利用分割点的单位向量，借助几何约束条件，生成最佳姿态假设。PVN3D和PointPoseNet都选择使用通过最远点采样选择关键点（FPS, Farthest Point Sampling）。与之相比，FFB6D (用于6D位姿估计的全流双向融合网络)\cite{he2021ffb6d56}提出了一种新的关键点选择算法SIFT-FPS，通过充分利用纹理和几何信息以取代FPS。SIFT-FPS主要包含两个步骤。第一步提取SIFT特征提取二维关键点。第二步，将2D关键点转换为3D关键点，并通过FPS对关键点进行选择。通过这种方式选取的关键点在物体表面的分布更均匀，纹理更清晰。最近，Wu等人\cite{wu2022vote57}认为以前的矢量或偏移方案对离散关键点敏感，因此提出RCV-POSE模型。在RCV-Pose中，引入了径向投票方案，对于每个目标点，使用一个深度学习模型来学习多个球面。每个关键点都可以位于球体的表面上。因此，在推理过程中，该算法只需利用这些球面的表面来投票选择一个三维累加器空间（3D accumulator space），累加器空间的峰值标示为关键点的位置。与以往的方法相比，RCV-Pose只需预测3个关键点，效率明显提高。

% 3）其他方法

% 除上述方法外，许多工作也表现出极大的兴趣，以设计模型满足某些特殊的要求，而不是仅仅为了提高精度。首先，一些方法试图解决由遮挡和杂乱等复杂场景引起的问题。例如，Alexander等人\cite{krull2015learning58}提出了一种学习比较观察到的RGB-D图像和渲染图像的方法，以更好地处理遮挡和复杂的传感器噪声，该方法通过用比较观察到的图像和渲染图像的卷积神经网络(CNN)描述特定物体姿态的后验密度来实现。Jafari等人\cite{hosseini2019ipose59}提出了一种基于实例分割和稠密坐标回归的三步分解方法来处理遮挡，该方法被称为第一个基于深度学习的局部遮挡物体精确姿态估计器。然后，Wang等人\cite{wang2019densefusion60}提出了DenseFusion，它充分利用了颜色和深度信息，从而在严重遮挡和复杂光照条件下具有鲁棒性。此外，Morefusion\cite{wada2020morefusion61}结合周围空间感知进行位姿预测，并联合多目标位姿优化，在遮挡严重、目标较小的杂乱场景中极大地提高了位姿估计的一致性和准确性。其次，除了处理复杂的场景外，最近的研究设计更轻量级的网络体系结构以提高实时性能。

%例如，Qi等人\cite{Qi2018}提出了一种区域级融合方法生成3D视锥减小目标候选区域的范围。华南理工大学的贾奎课题组在此基础上将3D视锥划分为网格单元，并在网格单元上应用卷积网络进行3D检测\cite{Wang2019}。Vora等人\cite{Vora2020}利用基于RGB图像的语义分割来增强输入点云，将增强后的点云送入3D目标检测器，获得了较好的检测结果。尽管像素级融合方法通过有效的预处理提高了检测性能，但是这种以顺序的方式执行多模态融合和3D目标检测的过程带来了不可忽视的时间成本；\textbf{第二类}，特征级融合方法。该方法在基于点云的3D目标检测器的中间阶段融合图像和点云特征。例如，在基于网格的检测网络骨干的中间层中，使用连续卷积(Continuous convolutions)\cite{Liang2018,Liang2019}、混合体素特征编码(Hybrid voxel feature encoding)\cite{Sindagi2019}和变换(Transformer)\cite{Zhang2022}网络等融合算子进行多模态融合。文献\cite{Chen2017}提出了一种多视角融合方法，在提议生成(Proposal generation)阶段进行多通道特征融合。这类方法建议对多模态表示进行更深层的融合，并产生更高质量的3D框，但在由于RGB和Depth图像生成的点云的特征本质上是异构的，因此在融合机制和视角对齐方面仍存在问题。


% 首先，RGB，Depth和光谱数据之间的融合方面存在以下问题：(1) 在数据表达上，多模态具有互补性，同时数据的异构性也带来了重复表达特征的问题，造成数据表示的冗余。(2) 在数据提取上，不同模态信息需要不同的处理方式，如何合理的从中提炼有效信息，也为数据提取带来了难度和增加工作量。(3) 在信息对齐上，不同信息的表达方式不同，需要从特性中找共性，才能在同一个系统中将二维图像、三维模型以及深度点云信息对齐。此外，因为数据样本中存在噪声，在噪声的干扰下，很显然是没有办法做到精准对齐的。(4) 在信息融合上，如何高效结合多个特征信息，而不是简单的叠加组合。 

% 其次，针对复杂环境下6D位姿估计问题的目标检测方法存在以下问题：(1) 在信息获取方面，单目、双目RGB相机在光照过强或过弱的情况下无法提供充分有效的目标信息,而点云特征在实际应用中难以保证检测的高精度与高效率。(2) 在可视化建模方面，传统的为所有目标注释分割掩膜(annotating segmentation masks)的方法非常复杂，并且在受到场景元素遮挡时该方法的可扩展性(scalability)将会受到限制。(3) 在特征提取方面，目标检测器专注于从真实边界框的中心区域抽取样本来预测边界参数，使其无法应对中心区域被其它目标或场景元素遮挡的情况。(4) 在推理预测方面，采用非最大抑制处理多个预测框的方法依赖于边框中心的小区域信息，忽视了刚体目标可见部分提供的信息。

% 再者，工业零部件呈现出弱纹理以及无纹理的特点，根据纹理特征进行6D姿态的估计是不切实际的。

% 针对上述问题，本课题研究复杂环境下适用的目标六自由度姿态估计算法以辅助危险设备智能装配，用深度学习的方法解决关键技术细节。

% \subsubsection{面向危险设备智能装配零部件六自由度姿态估计的多源融合表征的研究进展（刘志强）}

% 一般而言，多模态数据包含：图像数据、激光雷达数据、毫米波雷达数据、双目深度数据等。绝大多数传统研究将图像数据和激光雷达数据进行融合来提升3D目标检测精度。依据融合位置的不同，基于图像数据和激光雷达数据融合的多模态检测可以分为三类：第一类，前融合方法。该方法首先利用2D检测或分割网络从图像中提取知识，然后将图像知识传递给点云，最后将增强后的点云反馈给基于激光雷达的点云3D目标检测器。例如，文献\cite{Qi2018}提出了一种区域级融合方法生成3D视锥，并将其应用于激光雷达点云来减小目标候选区域的范围。华南理工大学的贾奎课题组在此基础上将3D视锥划分为网格单元，并在网格单元上应用卷积网络进行3D检测\cite{Wang2019}。文献\cite{Vora2020}利用基于图像的语义分割来增强输入点云，将增强后的点云送入激光雷达检测器，获得了较好的检测结果。除了语义分割，也有一些研究利用图像中的其他信息，例如使用深度图像来增强稀疏的3D点云\cite{Yin2021}。尽管前融合方法通过有效的预处理提高了检测性能，但是这种以顺序的方式执行多模态融合和3D目标检测的过程带来了不可忽视的时间成本；第二类，中融合方法。该方法在基于激光雷达的3D目标检测器的中间阶段融合图像和激光雷达特征。例如，在基于网格的检测网络骨干的中间层中，使用连续卷积(Continuous convolutions)\cite{Liang2018,Liang2019}、混合体素特征编码(Hybrid voxel feature encoding)\cite{Sindagi2019}和变换(Transformer)\cite{Zhang2022}网络等融合算子进行多模态融合。文献\cite{Chen2017}提出了一种多视角融合方法，在提议生成(Proposal generation)阶段进行多通道特征融合。这类方法建议对多模态表示进行更深层的融合，并产生更高质量的3D框，但在由于相机和激光雷达的特征本质上是异构的，因此在融合机制和视角对齐方面仍存在问题。第三类，后融合方法。该方法将图像和激光雷达数据分别进行单独处理，并将输出的2D和3D框融合\cite{Asvadi2018}。与前两类融合方法相比，该方法不仅可以更好的利用每个通道现有的网络，而且还避免了中间特征或输入点云上的复杂交互，拥有较高的执行效率。但是，由于输出结果不依赖于图像和激光雷达传感器的深度特征，这种方法无法利用不同模式的丰富语义信息，直到近几年才受到研究者关注\cite{Pang2020}。

% \subsubsection{针对危险设备智能装配零部件3D目标检测的研究进展（王庆元）}
% \textbf{2D目标检测的研究进展}

% 目前一些先进的6D姿态估计方法\cite{Di2021,Cai2022,Haugaard2022}通常需要使用2D目标检测器来为场景中的所有目标提取准确的2D边界框，获得感兴趣区域(Region of Interest, RoI)。现有的2D目标检测主要遵循两种策略：二阶段检测和一阶段检测。二阶段检测器首先使用区域建议网络(Region Proposal Network, RPN)来生成边界框候选，然后用分类和细化网络处理候选边界框以去除假正(False Positive, FP)样本并调整边界框的位置和大小\cite{He2020,Ren2017}。这种策略具有较高的检测精度，但是在实际应用中需要较高的成本并且计算效率较低。一阶段检测器在编码器最后的特征图中的每个空间位置用一组预定义的锚点(Anchor)代替区域建议网络\cite{Redmon2016}，取得了较高的计算效率。但是，预定义的锚点中存在大量的负(Negative)样本导致检测性能较差。文献\cite{Lin2017}通过焦点损失(Focal Loss)在一定程度上解决了样本数量不平衡问题，但是仍然没有达到二阶段检测器的精度。文献\cite{Zhang2020}在一阶段检测器中对正候选样本进行采样，有效地解决了负样本过多的问题。许多最新的检测方法遵循类似的策略\cite{Tian2022,Ge2021}，实现了比二阶段检测方法更加准确和高效的目标检测。这些方法在标准的目标检测基准测试中表现出了很好的性能，但是它们在6D位姿估计的基准测试中通常会受到严重遮挡等复杂因素的影响出现检测缺失或不准确的情况。

% \textbf{3D目标检测的研究进展}
% 主流3D目标检测方法依据数据模态可以分为三类：第一类，基于单目图像。由于相机能够提供充分的2D图像信息，一些研究使用发展成熟的2D目标检测的方法来实现3D目标检测，并且以较低的成本获得了令人满意的性能\cite{Mnih2015}。为了解决单目相机下三维信息无法获取的问题，文献\cite{Xu2018}采用了预先训练深度估计网络的方法生成深度图像，以获取深度感知特征来提高检测性能。一些研究尝试引入预先训练的辅助网络来学习图像中目标形状和几何一致性等先验信息来帮助准确定位3D目标。例如，文献\cite{Beker2020}从CAD模型中学习低维形状参数来获取特定类别的先验形状特征，提出一种渲染比较方法来学习3D目标的参数。然而，这种预先训练的网络模型需要不可忽视的注释成本，并且存在数据泛化问题。第二类，基于双目立体图像。与单目图像相比，双目立体图像可以提供准确的深度信息以及丰富的图像语义细节。例如，文献\cite{Peng2022}提出了一种新颖的立体检测框架来为成对的图像生成左右兴趣区域(Region of interests, RoIs)，然后融合RoIs来估计3D目标参数。文献\cite{Qian2020}将2D深度图转换为3D激光雷达点云，并且将3D深度估计网络与3D目标检测网络结合，使得整个网络能够端到端的训练。虽然双目立体图像能够提供更精确的目标定位信息，但是该方法仍然无法在光线不足、严重遮挡等恶劣条件下工作。第三类，基于点云。点云数据是一种稀疏且不规则的3D表示，因其能够提供丰富的环境信息以及物体的几何特征，在3D目标检测领域受到广泛关注。根据模型的检测过程，可以分为单阶段检测和双阶段检测。单阶段检测是指从原始的点云数据中获取特征并直接预测目标的3D边界框。研究人员使用不同的数据预处理技术来处理稀疏的点云数据以便后续的主干网络获取点云特征，例如基于点的3D目标检测\cite{Pan2021}、基于体素的3D目标检测\cite{Zhou2019}、基于图的3D目标检测\cite{Shi2020}等。双阶段检测方法是指在第一阶段生成3D候选目标，在第二阶段对第一阶段中生成的候选目标进行细化。文献\cite{Shi2020a}是一项开创性研究，该方法在第一阶段将点云划分为球形体素进而提取点云特征并生成3D目标提议，在第二阶段输入点云采样中的关键点，利用新的点算子来获得更加丰富的特征。文献\cite{Shi2022}在此基础上使用新型的运算方法来改进第二阶段的模块。然而在实际应用中，如何保证高精度3D目标检测同时保持高效率仍然是该研究领域面临的一个挑战。

% 上述研究均取得了被领域认可的检测结果，为本课题研究提供了理论基础，但是面对复杂环境中6D位姿估计问题，仍然进一步完善来获得更加精确的目标检测：首先，单目、双目相机在光照过强或过弱的情况下无法提供充分有效的目标信息；其次，传统的为所有目标注释分割掩膜的可视化建模方法非常复杂，并且在受到场景元素遮挡时该方法的可扩展性将会受到限制；再者，单阶段2D目标检测忽视了刚体目标可见部分提供的信息，专注于从真实边界框的中心区域抽取样本来预测边界参数使其无法应对中心区域被其它目标或场景元素遮挡的情况；另外，在推理预测过程中采用非最大抑制处理多个预测框的方法依赖于边框中心的小区域信息，忽视了刚体目标可见部分提供的信息。本课题尝试融合光谱特征来提供准确的目标信息，并有效利用刚体可见部分的特征来对3D边界框进行预测。

% \subsubsection{针对危险设备智能装配零部件目标六自由度姿态估计研究进展（海洋）}

% 基于CNN的6D姿态估计方法可以被划分为以下几种：（1）基于2D-3D对应关系，进一步结合RANSAC-PnP来求解物体的6D姿态。该类方法由于其在网络学习过程可以提供显式明确的监督信号，在6D姿态估计领域中显示出了优良的性能。其中该类方法可以进一步划分为基于关键点的方法，即稀疏的3D-to-2D对应关系\cite{Peng2019,Hu2019,Hu2021}，以及基于稠密2D-to-3D对应关系的方法\cite{Li2019,Park2019,Su2022}。文献\cite{Peng2019}提出通过物体可见掩膜上的每个点预测到关键点投影的方向，进而通过投票的方法来确定关键点投影的位置。文献\cite{Xiang2018} 对物体的三维模型进行分层编码，预测每个物体可见掩膜上每个像素对应3D点的编码，将回归问题转化为分类问题，在大多数6D姿态估计基准上创造了新的记录。（2）直接回归6D姿态的方法。该类方法端到端地估计目标物体的6D姿态。然而由于6D姿态回归的歧义性以及无法为网络提供足够直接的监督信号，该类方法在性能上劣于基于2D-3D对应关系的方法。文献\cite{Wang2021}提出使用稠密的2D-to-3D对应关系作为中间表示，来协助6D姿态的回归。文献\cite{Hu2020} 提出使用PointNet神经网络来近似作用在稀疏的3D-to-2D对应关系的PnP。近期，基于CNN的6D姿态修正方法获得了越来越多学者的关注，该类方法的目的是使用一个单独的网络来修正姿态估计网络估计的不精确的6D姿态。由于其迭代优化的范式，该类方法可以在性能和精度方面进行权衡。文献\cite{Hu2022}证明基于光流的6D姿态修正方法具有优良的领域泛化能力，即在合成图像上进行训练，即可泛化到真实图像上。文献\cite{Lipson2022}进一步结合多视图与可微分捆绑调整层来使姿态修正网络可以利用深度图像来训练与测试。


\fi 

{
\bibliographystyle{IEEEtran}
\bibliography{nsfc_sr}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NsfcSection{2}{项目的研究内容、研究目标，以及拟解决的关键科学问题}{
（此部分为重点阐述内容）；}

\subsection{研究目标}
针对危化品智能装配中6D位姿估计的三个核心问题开展研究，通过融合双目和多光谱相机的数据，用多源融合表征的方法解决工件表面无纹理且具有情况下的目标检测问题，通过xx提升有遮挡情况下的目标检测精度，通过设计CAD模型几何特征引导的修正组建，改进姿态修正的准确度，对改进策略进行有机融合，提升6D姿态估计的准确度，满足危化品智能装配对工件6D姿态估计精度的要求。


%使用双目相机和高光谱仪获取目标零部件RGB数据、Depth数据以及光谱数据，将多模态学习引入到危化品智能装配零部件的六维位姿估计中，展开理论方法和应用研究。具体目标包括：(1)面向危化品智能装配零部件的多源融合表征(2)针对危化品智能装配零部件复杂环境下的3D目标检测(3)面向危化品智能装配基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计



\subsection{研究内容}

为了解决同类研究在真实复杂环境下暴露的不足，本课题组研究涉及危险设备智能装配环境下基于几何特征引导的目标六自由度姿态估计问题，并融入多源融合表征以及多模态3D目标检测框架。研究内容可提炼为三个方面，它们互相联系构成完整的面向危险设备智能装配零部件的六自由度姿态估计系统。

\textbf{A、面向危险设备智能装配零部件的多源融合表征}
针对危险设备智能装配零部件的多源融合表征存在的问题和挑战，以融合方案对位姿估计优化后的精度为评价依据，研究多模态信息的表达和特征提取。研究该过程包含两个细节问题：（1）针对目标数据的差异性以及不同模态数据对于姿态估计任务的重要性，解决姿态估计问题中多种信息表达的关键问题，制定高效针对性的目标特征提取策略；（2）研究神经网络下多模态特征的信息对齐和融合问题，提出可行的多模态信息融合方式。

\textbf{B、针对危险设备智能装配零部件复杂环境下的3D目标检测}
针对复杂场景下目标检测算法存在的问题和挑战，研究利用刚性目标为边框预测提供的有效信息来提高检测进度。首先，研究利用目标边界框中的所有像素来指导生成可见性(visibility)掩膜；然后，使用可见性掩膜指导训练期间对候选目标的抽样，使得网络由所有可见部分监督并且丢弃遮挡部分；最后，研究融合所有高可靠性候选局部预测，以产生更加鲁棒的检测结果。

\textbf{C、面向工业机械臂抓取基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计}
基于检测-估计-修正的6D姿态估计框架显示出了优良的性能，被近期提出的大多数方法所采用。采用一个单独的6D姿态修正网络可以放松对前一级估计网络的要求。同时基于CNN的6D姿态修正网络的输入为目标图像与在前一级估计网络得到的粗糙姿态下的渲染图像，这样的输入可以减轻姿态修正网络对数据的需求。目前大多数方法通过估计目标图像与渲染图像之间的对应关系来求解目标图像中物体的6D姿态。然而这种方法很强地依赖于物体的纹理特征，而对于工业零部件，大部分物体呈现出无纹理或者弱纹理的特征，因此这种方法应用于工业零部件上会有很强的局限性。因此对于无纹理或者弱纹理的工业零部件，物体的几何特征需要被有效地利用，然而现有的姿态修正方法很少探索物体的几何特征。
本课题拟提出一个基于物体几何特征的6D姿态修正方法。在现有的基于纹理特征的对应关系估计网络中，如RAFT，本文通过利用点云特征提取网络，显式地将物体的几何特征进行编码，作为对应关系估计网络的参考信息。在迭代优化过程中，对应关系估计网络全局地通过可微姿态求解层来修正在上一步迭代过程中求解得到的姿态。具体来说，将物体的几何特征在上一步估计得到的姿态下进行投影，同时将物体的几何信息引入目标图像与渲染图像的特征匹配过程中，结合特征匹配结果与投影得到的几何特征来全局地求解更新之后的目标物体的6D姿态。



\subsection{拟解决的关键科学问题}

\textbf{A. 面向危险设备智能装配零部件的多源融合表征}
如立论部分所述，在姿态估计中加入多种信息，理论上是有利于最终结果的评估，但是在实际处理中，各种模态信息具有各自的信息特征，将多种信息有效的融合则是高效解决问题的关键。针对危险设备智能装配环境，引入光谱数据区分火工品装配危险区与非危险区。针对目标数据的差异性，制定高效鲁棒的目标特征提取策略，同时考虑多模态特征的信息对齐和融合。

\textbf{B. 针对危险设备智能装配零部件复杂环境下的3D目标检测}
如立论部分所述，复杂场景中的目标存在严重的遮挡问题，这使得专注于学习从真实边界框中心区域抽取的样本来预测边界参数的方法就不再适用，有效利用刚性目标对象的可见部分来获取目标边框的可靠估计是解决该问题的关键。本课题利用边界框中所有像素信息来获得目标可见部分的掩膜，取代了传统复杂的标注分割掩膜的可视化方法。针对目标中心遮挡情况下无法获取有效样本问题，在推理过程中融合所有高可靠性候选局部预测，以获得更准确的检测结果。

\textbf{C. 面向危险设备智能装配基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计}
如立论部分所述，对于弱纹理的物体，当前基于纹理特征的6D姿态修正方法是不适用的。课题组前期对此进行了实验分析。如图所示，对于图中的弱纹理物体，基于纹理特征估计的渲染图像到目标图像之间的2D对应关系较差，无法保持物体的形状，导致最终通过RANSAC-PnP估计得到的姿态精度较差。尽管图中所示的物体不属于工业零部件，但是其无纹理的特征与工业零部件是相同的。通过该例子引申出工业零部件的6D姿态修正难点所在，由于工业零部件的弱/无纹理特征，在对零部件拍摄得到的图像中，大部分物体上的 像素呈现出一致的纹理特征，不具有区分度。在这种情况下，通过上下文信息以及几何特征感知像素在物体几何模型上的位置，进而估计2D对应关系是可行的解决方案。


\NsfcSection{3}{拟采取的研究方案及可行性分析}{
（包括研究方法、技术路线、实验手段、关键技术等说明）；}


\subsection{拟采取的技术路线}
本课题研究的面向危险设备智能装配零部件六自由度姿态估计的总体方案示于图\ref{fig:sys_arch}。其中涉及的三个关键问题的详细方案和可行性分析分别阐述如下：

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{sys_arch.png}
    \end{overpic}
    \caption{本课题研究方案的总体框架
    }\label{fig:sys_arch}
\end{figure}

\textbf{A.面向危险设备智能装配零部件的多源融合表征设计方案及可行性分析}

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{feature_biflow_fusion.png}
    \end{overpic}
    \caption{特征级双向全流多源融合
    }\label{fig:feature_biflow_fusion}
\end{figure}

输入部分为三种模态数据：2D彩色图、2.5D深度图和光谱图像。根据数据表达的意义采取不同的处理手段：对于2D彩色图像，表达目标实时纹理颜色，直接提取二维语义特征；对于2.5D深度图片，表达目标上各点到相机的距离，表达为三维点云更充分保留空间信息，提取空间特征；对于光谱图像，在危险设备装配环境中能更加有效识别目标，作为辅助数据优化结果。整个目标姿态估计网络框架分为四个部分：融合UV输入、特征提取、特征共享和特征融合。

\textbf{B. 针对危险设备智能装配零部件复杂环境下的3D目标检测网络设计方案及可行性分析}（王庆元）
\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{visable_guided_3d_detect.jpg}
    \end{overpic}
    \caption{可见性引导的3D目标检测流程图
    }\label{fig:visable_guided_3d_detect}
\end{figure}

该关键问题的研究重点着眼于复杂环境下刚性目标可见部分引导检测。首先，对真实刚性目标边界框进行裁剪来得到图像块，依据刚性目标的可见部分与边界部分像素之间的显著差异性，设计一种可见性距离函数并计算图像块内所有像素与边界距离来构建目标的可见性掩膜；然后使用主干网路对融合特征进行特征提取来生成特征图，依据每个特征单元对应所有像素的可见性距离来计算特征单元的可见性分数，设置可见性分数阈值对所有的特征单元进行筛选采样获得候选正单元；最后，研究特征单元预测框的置信度，并根据置信度融合所有的候选预测以获得更加鲁棒和精确的检测的结果。
一阶段目标检测器通常使用特征金字塔(FPN)输出多种比例的特征图，将特征图中的特征向量作为训练样本进行分类和回归处理。在检测训练期间，需要为每个注释的目标实例定义正样本和负样本，并使用正样本回归目标实例的边界框参数。因此这一框架的关键是在训练期间有效的选择正样本。本课题的前期预研中，在常规的目标数据集(COCO)和典型的6D目标位姿数据集(YCB-V)上，将真实目标掩膜内随机采样一定数量的正单元策略与两种基准策略进行评估对比，包括从真实边界框(FCOSv2)的中心区域采样固定数量的正单元，以及跨所有金字塔特征层级的自适应中心采样策略(ATSS)。图4显示了使用这些采样策略获得的不同局部预测的平均测试精度。对于常规的COCO数据集，无论在训练期间使用哪种采样策略，精度都会随着距离的增加而恶化。这因为COCO数据集中包括许多非刚体目标和具有相同目标类型的各种实例，使得基于中心区域采样的方法也可以获得可靠的边框预测结果。对于YCB-V数据集，由于大多数非中心区域特征没有参与训练，基于中心区域采样的策略的准确性也随着距离的增加而快速下降。然而，由于YCB-V目标的刚性，可见性引导(visibility guided)策略即使是对于那些远离中心区域的预测，也能取得更准确的局部预测。本课题将把该方法从2D边界框回归拓展到3D目标检测领域。

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{rigid_detect_analysis.png}
    \end{overpic}
    \caption{刚性检测分析
    }\label{fig:rigid_detect_analysis}
\end{figure}

我们展示了不同抽样策略的检验精度。在典型的一般对象数据集(COCO[29])和典型的6D目标位姿数据集(YCB-V)上的训练期间，不同的局部预测。图中显示了FCOSv2(Center)、ATSS(Center+)的结果，以及利用真实掩膜中的所有候选框(Visible)策略的结果。水平轴表示局部预测到边框中心的归一化距离。

\textbf{C. 面向危险设备智能装配基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计方案及可行性分析（海洋）}

本方案的关键问题在于对针对目标物体估计得到的不精确的6D姿态进行修正。利用该6D姿态在图像对物体进行定位，进而对目标物体所在的区域进行放大，同时减轻其他物体以及背景对姿态修正的影响。该方案包括几个部分，首先利用卷积神经网络对在不精确的6D姿态下渲染的初始图像以及目标图像进行特征提取，接下来迭代地进行6D姿态的修正，借助于RAFT中提出的相关性查找组件，计算在估计得到的初始图像与目标图像之间密集的2D对应关系（即光流）下的相似度。利用该相似度，本方案进一步估计对该对应关系的修正。为了将物体的几何信息引入特征匹配过程，本方案根据修正之后的密集对应关系，通过姿态估计子网络直接回归6D姿态。接下来本方案将物体的CAD模型在该6D姿态下进行投影，得到几何信息驱动的对应关系，以作用于下一次迭代中的相关性查找中。

随着RAFT的提出，基于RAFT中迭代修正框架在许多领域展现出了巨大的潜力。文献[10]进一步成功地将RAFT应用到6D姿态修正领域，并展现出了优良的性能。这是本项目分析可行性的重要依据。本项目在RAFT的基础上，将物体的几何信息引入特征匹配过程，使得匹配过程结合纹理信息与几何信息，不仅可以减小匹配过程中的搜索范围，同时几何信息的引入使得无/弱纹理物体的姿态修正更加鲁棒。

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{geo_guided_6D_refine.png}
    \end{overpic}
    \caption{基于几何特征引导的6D姿态修正框架
    }\label{fig:geo_guided_6D_refine}
\end{figure}

\subsection{可行性分析}


\NsfcSection{4}{本项目的特色与创新之处；}{}

\textbf{A. 多源融合表征的特色与创新之处}
一方面，通过引入额外的UV数据保持深度图点特征三维空间位置的一致性，由此可通过参数共享的神经网络架构，在RGB和Depth特征提取阶段，通过全流双向融合各自互补特征。另一方面，通过引入光谱图像，针对危险设备智能装配零部件不同地光谱特征响应，可以更准确和鲁棒地提取装配零部件实例级目标特征。

\textbf{B. 3D目标检测的特色与创新之处}
引入刚性感知检测方法，提出了一种新颖的可见性图来取代掩膜标注，并利用可见性图指导网络在训练期间对可见区域样本采样；考虑同一实例的不同尺度给检测带来的影响，引入尺度感知器来对融合特征不同尺度的特征图进行处理，提高了模型的特征提取与利用能力，并结合2D特征图采样结果来指导3D检测框回归；融合所有高于置信度阈值的候选局部预测，从而产生更鲁棒的检测结果。

\textbf{C. 基于目标几何特征引导的6D位姿估计的特色与创新之处}
通过可微相对姿态求解层逐步优化估计的6D姿态，并将几何信息引入特征匹配过程，使得匹配得到的特征与物体的几何形状一致。进一步显式地使用深度神经网络编码几何特征作为姿态修正网络的参考信息。

\NsfcSection{5}{年度研究计划及预期研究结果}{
（包括拟组织的重要学术交流活动、国际合作与交流计划等）。}

\subsection{年度研究计划}

本课题研究期限从2023年1月到2026年12月。年度计划如下：
2023.01-2023.06		总体方案制定和论证
查阅计算机视觉领域和机器人领域国际知名期刊和会议的最新文献
开始对课题组的博士生和研究生进行python，pytorch以及三维视觉的入门培训
撰写算法设计书
2023.07-2023.12		三维视觉感知硬件平台搭建
搭建三维视觉硬件平台
搭建算法训练测试服务器平台，完善课题组原有服务器实验平台
2024.01-2024.06		多源融合表征建模研究
研究内容1：面向危险设备智能装配零部件的多源融合表征
拟参加国际会议学术交流一次，拟完成国际期刊论文1~2篇，专利1项
2024.07-2024.12		3D目标检测算法研究
研究内容2：针对危险设备智能装配零部件复杂环境下的3D目标检测
拟完成国际期刊论文1~2篇，专利1项
2025.01-2025.06		六自由度姿态估计算法研究
研究内容3：面向工业机械臂抓取基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计
拟完成国际期刊论文1~2篇，专利1项
2025.07-2025.12		各算法模块的优化
多源融合，目标检测和六维位姿估计算法形成模块，并在软硬件平台上进行验证和完善
拟完成国际期刊论文1~2篇
2026.01-2026.08		各算法模块的联合
多源融合，目标检测和六维位姿估计算法整合成完整的针对危险设备智能装配零部件目标六维位姿估计系统，并在软硬件平台上进行测试和完善
拟完成国际期刊论文1~2篇
2026.09-2026.12		准备课题评审
整理研究成果
撰写课题总结报告

\subsection{预期研究成果}

本课题将提出适用于危险设备智能装配零部件六自由度姿态估计算法。并基于该理论的研究成果，在国际学术期刊、国际学术会议和国内一级刊物上发表论文6-8篇；申请专利1-2项；培养博士研究生1-2名，硕士研究生3-5名。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ContentDes{（二）研究基础与工作条件}


\NsfcSection{1}{研究基础}{
（与本项目相关的研究工作积累和已取得的研究工作成绩）；}

\subsection{工作基础1}

在6D位姿估计方面，在前期工作中，课题组积累了大量公开数据集，同时在计算机视觉顶级会议ECCV 2022举办的BOP 6D位姿估计挑战赛中获得了最佳单模型奖项，提出的方法可以同时处理单目RGB以及RGB-D图像，可扩展性较强。

\subsection{工作基础2}



\subsection{研究工作获奖}


\NsfcSection{2}{工作条件}{
（包括已具备的实验条件，尚缺少的实验条件和拟解决的途径，
包括利用国家实验室、
国家重点实验室和部门重点实验室等研究基地的计划与落实情况）；}


\myPara{经费和硬件条件方面}我们


\myPara{人员方面}我们

\myPara{国内外合作方面} 我们


\NsfcSection{3}{正在承担的与本项目相关的科研项目情况}{
（申请人和项目组主要参与者正在承担的与本项目相关的科研项目情况，
包括国家自然科学基金的项目和国家其他科技计划项目，
要注明项目的名称和编号、经费来源、起止年月、与本项目的关系及负责的内容等）；}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ContentDes{（三） 其他需要说明的问题}



\NsfcSection{1}{}{
申请人同年申请不同类型的国家自然科学基金项目情况
（列明同年申请的其他项目的项目类型、项目名称信息，
并说明与本项目之间的区别与联系）。}


\NsfcSection{2}{}{
具有高级专业技术职务（职称）的申请人或者主要参与者是否存在
同年申请或者参与申请国家自然科学基金项目的单位不一致的情况；
如存在上述情况，列明所涉及人员的姓名，
申请或参与申请的其他项目的项目类型、项目名称、单位名称、
上述人员在该项目中是申请人还是参与者，并说明单位不一致原因。}



\NsfcSection{3}{}{
具有高级专业技术职务（职称）的申请人或者主要参与者是否具有
高级专业技术职务（职称）的申请人或者主要参与者是否存在与正
在承担的国家自然科学基金项目的单位不一致的情况；如存在上述情况，
列明所涉及人员的姓名，正在承担项目的批准号、项目类型、项目名称、
单位名称、起止年月，并说明单位不一致原因。}


\NsfcSection{4}{}{其他。}

无


\end{document}
