% https://www.overleaf.com/read/jydxqkkkskzp
% https://github.com/MCG-NKU/NSFC-LaTex
% by Ming-Ming Cheng https://mmcheng.net

\documentclass[12pt]{article}
\usepackage[UTF8]{ctex}
\usepackage{nsfc}

\newcommand{\note}[1]{\textcolor[rgb]{0.6,0,0}{note: #1}}
\newcommand{\todo}[1]{{\textcolor{red}{\bf [#1]}}}
\newcommand{\myPara}[1]{\paragraph{#1：}}

\graphicspath{{figures/}}


\begin{document}



%%%%%%%%% TITLE

\title{报告正文：危化品智能装配中的高精度6D姿态估计方法研究}

\maketitle

\ContentDes{（一）立项依据与研究内容（建议8000字以下）：}


\NsfcSection{1}{项目的立项依据}{
（研究意义、国内外研究现状及发展动态分析，需结合科学研究发展趋势来论述科学意义；或结合国民经济和社会发展中迫切需要解决的关键科技问题来论述其应用前景。
附主要参考文献目录）；}

\subsection{研究意义}

爆炸品、有毒化工、放射性物质、生物化学制剂等危化品的装配工艺，对于制备环境要求很苛刻。工厂装配过程中，为了确保装配安全性，对产线工人的年龄、工作时间和操作流程等都有严格的要求。以工程炸药制备公司为例，为了保障安全生产，工人正常上工前需培训6个月，每天工作时间不能超过6小时，每项工艺流程需双人确认。这些安全措施极大限制了工厂的产能，亟需通过智能制造技术替代人工，提升产能。

% 此处表述6D姿态估计是智能装配中的关键技术，为了提升精度，需要用多传感器融合
待装配物件的六自由度（6D）姿态估计是智能装配的关键技术，其核心问题是高精度的姿态估计。与普通的工业自动化设备不同，危化品的装配操作流程裕度很小，无法通过降低机械操控设备的控制精度和重访精度实现无人操作。智能装配工艺通过装置在产线上的传感器，估计炸药、放射性元素、腐蚀性药品等空腔的姿态，在操控链路形成负反馈，精准完成原料装填和定位螺丝的紧固。

% 传统的6D姿态估计采用RGB或RGBD的方式计算姿态，准确度以目标物体点云的10%作为度量，无法满足精准装配的需要，亟需改进；当前的学术研究中，6D姿态估计算法估计的准确度以目标物体点云尺度的10\%作为度量，其精度只适用于普通物件如快递包裹、日常物品等的粗抓取，无法满足危化品装配的精细操控要求。
面向危化品智能装配的6D姿态估计需要解决三个核心问题，其一为无纹理表面的算法适应性问题，其二为有遮挡情况下的鲁棒估计问题，其三为姿态估计的高精度修正问题。本项目融合双目相机和光谱相机的影像特征，通过分析多模态传感器数据的特征表达，提取和对齐不同模态的特征，并通过交叉验证抑制单一模态中的噪声，融合有效的互补信息，提升6D姿态估计的适应性、鲁棒性和精度，解决危化品智能装配中的高精度姿态估计难题。

\subsection{国内外相关工作}

大多数姿态估计方法遵循两阶段的范式，首先检测图像中的目标物体，之后在缩放后的目标物体区域上进行姿态估计。尽管现有方法在大多数简单的场景下表现良好，但由于所使用的检测器对无纹理或被遮挡工件的检测效果并不理想，算法在智能装配应用中的性能急剧下降。

对于第一阶段的目标检测任务，常用的检测方法有两段式和单段式两类\cite{ATSS, fcosv1, fcosv2, PAA, faster-rcnn, maskrcnn}。两段式检测方法首先采用区域建议网络~\cite{faster-rcnn, maskrcnn}生成边界框候选体，然后由分类和细化网络处理，去除假阳性，并调整边界框的位置和大小。这种策略准确度较高，但成本高，效率低。单段式检测器通过在编码器的最终特征图中的每个空间位置用一组预定义的锚框代替区域建议网络来解决这个问题~\cite{retinanet,fcosv1,yolov1}。这种方法会导致锚点中存在大量负样本，虽然这一问题可以通过focal loss~\cite{retinanet,fpn}在一定程度上解决，但程度有限，早期的单段式检测器并没有达到两段式检测器的精度。Zhang~\cite{ATSS}通过一个简单而有效的正样本策略解决了这一问题。最近的大多数检测方法都遵循类似的策略~\cite{fcosv2, PAA, autoassign, OTA, TTF, yolov3}，相关改进算法的准确性比两段式方法更好，同时效率更高。但即便如此，这些方法都假设场景中物体的纹理相对丰富，且具有较少的遮挡，与智能装配的6D物体姿态估计场景中的特性仍有较大差异。

% 无纹理物体位姿估计解决方案：在数据输入源头上解决，即增加depth图像和高光谱图像，实现多源融合。
危化品工件的表面大多没有纹理，这种工况下的目标检测是姿态估计中的一个难点。仅通过RGB图像估计的位姿精度不高，鲁棒性较差。一方面，深度学习网络很难从单幅RGB图像中提取有效的颜色和几何特征，另一方面，三维物体到二维像平面的投影过程丢失了三维结构的几何约束，单幅图像无法逆推结构信息。
基于RGB-D的6D位姿估计方法可以利用点云的三维几何特征来预测目标姿态，其估计精度比单幅RGB图像更高。这类算法的难点在于RGB图和深度图两种异构数据的融和。目前解决的思路有三类。
第一类为像素级融合方法，先利用2D检测或分割网络提取RGB图像特征，将图像特征传递给深度图生成的点云，然后将增强后的点云反馈给点云3D目标检测器。2D检测的结果可辅助三维点云形成3D视锥\cite{Qi2018}，减小候选区域的范围。所形成的的视锥也可以进一步划分为网格单元\cite{Wang2019}进行3D检测。或者2D分割的结果也可用于增强3D点云\cite{Vora2020}，将增强后的点云输入3D目标检测器提升检测效果。这类方法以顺序的方式进行融合，效率相对较低。
第二类为特征级融合方法，在基于点云的3D目标检测器的中间阶段融合图像和点云特征。例如，在基于网格的检测网络骨干的中间层中使用连续卷积\cite{Liang2018, Liang2019}、混合体素特征编码\cite{Sindagi2019}或Transformer\cite{Zhang2022}网络等融合算子进行多模态融合。
第三类为决策级融合，将RGB图像和Depth图像生成的点云数据通过两个独立网络分别生成2D和3D检测框\cite{Asvadi2018}并融合输出。这种方法可以更好地借鉴每个独立任务的SOTA算法，避免中间特征层上的信息交互，执行效率高，但无法利用不同模式的互补语义信息\cite{Pang2020}提升检测精度。

%本项目在特征级融合的基础上，引入UV数据保持深度图点特征三维空间位置的一致性，从而在异构输入源之间解决视角对齐的问题。
与普通工业零件不同，危化品工件中通常有危险度较高的化学部件，如易爆、有毒或有辐射的填充物。对于这类区域的精细检测和分割，一方面可以避免误操作减少事故，另一方面可为检测网络提供更精准的边界区域特征。不同的化学元素在特定的光谱谱段有脉冲响应特性，可通过光谱相机清晰地定位物质边界，能显著提升姿态估计第一阶段的检测和分割精度。光谱图像的检测和分割算法思路与可见光图像相似，最主要的问题在于样本数据较少，且分布不均匀，业界没有大规模数据集可用于网络训练。目前解决这一问题的主要方法是小样本学习\cite{lys2022targetDetection, shi2020HyperspectralTargetDetection}或加强注意力机制\cite{shi2020hyperspectralROI}。

% 此处描述 RGB, Depth, 和光谱数据融合的国内外现状
对于同一场景下的可见光、深度图和光谱图像的融合处理，目前没有可直接借鉴的方法，但多传感器融合的方法在遥感影像处理和自动驾驶领域中都有相关研究。一种朴素的融合方法是决策级融合，对不同的输入数据分别提取特征图\cite{pang2020clocs}，构建基于统计的特征加权组合模块，对不同输入分支提取的特征赋予不同的权重，在输出层进行决策级融合\cite{li2022sal}。这种方法既可以区分不同输入源特征在分类器中的重要性，又可以避免特征提取网络学习本身对分类结果的干扰。但它没有考虑多源数据特征间的差异性与互补性。另一种思路是特征级融合，在对不同输入源的数据进行特征抽取时，增加三维交叉注意力模块\cite{li2022triplet}增强多源数据的互补空间特征，并进行逐层特征融合。这种方法在自动驾驶领域\cite{huang2020epnet}取得了不错的结果。

% 有遮挡情况下的鲁棒估计现状
遮挡是6D姿态估计中的另一个难点问题，通用的目标检测方法假设物体之间遮挡较少，标准真值包围框中心的区域周围为目标物体，因此在网络学习中专注于仅从这些区域提取的样本预测边界框参数。然而，在有遮挡的情况下，真值包围框的中心通常被其他物体或者场景元素遮蔽，导致检测框出现较大偏差。为了提升姿态估计的性能，大多数方法需要依赖额外的姿态修正组件。这些组件首先根据初始姿态以及物体的CAD模型渲染合成图像，然后基于光流网络估计渲染图像和输入目标图像之间的密集2D-to-2D对应关系。在利用目标的3D形状信息将2D对应关系提升到3D-to-2D对应关系之后，基于PnP算法迭代计算更精细的姿态参数。

这种算法框架在大多数通用场景下表现良好，但它有几个缺点。
首先，所使用的光流网络建立在两个假设之上，即两个潜在匹配之间的亮度一致性和本地邻居内匹配的平滑度。这一假设在通用场景下是成立的，但在智能装配的6D物体姿态估计场景下，我们没有关于目标形状的线索，缺少形状约束，从而导致目标图像中每个像素的潜在匹配空间盲目扩大。
其次，匹配过程中物体形状信息的缺失会导致匹配结果出现明显偏差，在PnP姿态求解过程中引入显著噪声。
另外，在这种两阶段的框架中，第一阶段的训练依赖于匹配网络的损失函数，该匹配损失不能直接反映最终的6D姿态估计损失，且不是端到端可训练的。

近几年的位姿估计方法，通过使网络预测一些预定义的3D关键点~\cite{rad2017bb8, hu2019segDriven, peng2019pvnet, Hu2021}，或对每个2D像素预测稠密的对应3D点~\cite{zakharov2019dpod, Su2022, li2019cdpn, wang2021gdrnet, Di2021}来创建对应关系。之后通过数值PnP求解器~\cite{lepetit2009epnp}或直接从中间对应关系的表达来学习姿态~\cite{hu2020singleStage, EroPnP,wang2021gdrnet, Di2021}。通过精心设计卷积神经网络结构改进的算法~\cite{he2016resnet, resnext_2017_cvpr}在鲁棒性和准确性方面都有显著提升~\cite{Xiang2018, peng2019pvnet, wang2019densefusion60}，但复杂的杂乱场景下，姿态估计的准确性依然不高。

对于6D姿态的修正，以往的方法主要依赖于已配准的深度图~\cite{Xiang2018, li2019cdpn, wang2019densefusion60}，但在许多真实场景~\cite{Hu2021}中深度图难以直接获取。
近几年的姿态修正方法使用无需深度数据的“渲染-比较”策略，可以获得性能相当或更好的结果~\cite{li2018deepim, zakharov2019dpod, cosypose, rad2017bb8, Hu2022, Lipson2022, RNNPose_2022_cvpr,Repose_2021_iccv}。Hu等~\cite{Hu2022}近期提出的6D姿态修正方法与这一策略有所不同，他将6D姿态修正问题建模为渲染图像到目标图像之间的2D匹配关系问题，通过数值求解获得修正之后的姿态。算法精度比“渲染-比较”的策略更高，但由于该方法将6D姿态修正建模为无约束的纯2D到2D匹配问题，脱离了6D物体姿态的物理意义，因此理论上讲估计结果是次优的。为了提升性能，申请人团队提出了一个由目标的3D形状引导的形状约束递归匹配框架，在精度和效率方面都有了大幅提升\cite{?}。

% 为了解决当前检测器在遮挡环境下的退化问题，我们提出了一种检测方法，该方法利用了6D姿态估计中目标对象是刚性的这一特性。
% 对于此类对象，任何可见部分都可以提供完整边界框的可靠估计。因此，我们认为，与标准物体检测器使用的基于中心的采样相比，任何从可见部分提取的特征都应该是训练期间正样本的潜在候选。
对于危化品智能装配中的三个核心问题，每个细分学术领域的学者及申请人团队都有相关的基础研究。在多模融合解决无纹理物体的检测方面，可见光和深度图像融合估计已有可借鉴的算法框架，但如何有效融合光谱数据仍需探索。有遮挡下的鲁棒估计是目前6D姿态估计中的一个公认难题，通过改进网络结构和融合策略提升鲁棒性目前算法改进的主要思路。而对于高精度姿态修正问题，基于几何引导的约束框架是目前探索的行之有效的解决方案，但仍有很大的改进空间。

{
\bibliographystyle{IEEEtran}
\bibliography{nsfc_sr}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NsfcSection{2}{项目的研究内容、研究目标，以及拟解决的关键科学问题}{
（此部分为重点阐述内容）；}

\subsection{研究目标}
针对危化品智能装配中6D位姿估计的三个核心问题开展研究。通过融合双目和光谱相机的数据，用多源融合表征的方法解决工件表面无纹理情况下的目标检测问题，通过刚性感知检测提升有遮挡情况下的目标检测精度，通过设计CAD模型几何特征引导的修正组建，改进姿态修正的准确度。对改进策略进行有机融合，提升6D姿态估计的准确度，满足危化品智能装配对工件6D姿态估计精度的要求。


%使用双目相机和高光谱仪获取目标零部件RGB数据、Depth数据以及光谱数据，将多模态学习引入到危化品智能装配零部件的六维位姿估计中，展开理论方法和应用研究。具体目标包括：(1)面向危化品智能装配零部件的多源融合表征(2)针对危化品智能装配零部件复杂环境下的3D目标检测(3)面向危化品智能装配基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计



\subsection{研究内容}

为了解决同类研究在真实复杂环境下暴露的不足，本课题组研究涉及危险设备智能装配环境下基于几何特征引导的目标六自由度姿态估计问题，并融入多源融合表征以及多模态3D目标检测框架。研究内容可提炼为三个方面，它们互相联系构成完整的面向危险设备智能装配零部件的六自由度姿态估计系统。

\textbf{A、面向危险设备智能装配零部件的多源融合表征}
针对危险设备智能装配零部件的多源融合表征多模态对齐融合的问题和挑战，以融合方案对位姿估计优化后的精度为评价依据，研究多模态信息的表达和特征提取。研究该过程包含两个细节问题：（1）针对目标数据的差异性以及不同模态数据对于姿态估计任务的重要性，解决姿态估计问题中多种信息表达的关键问题，制定高效针对性的目标特征提取策略；（2）研究神经网络下多模态特征的信息对齐和融合问题，提出可行的多模态信息融合方式。

\textbf{B、针对危险设备智能装配零部件复杂环境下的3D目标检测}
主流的目标检测算法在通用数据集上可以实现高精度、高实时性的目标检测。这类方法通常使用特征金字塔网络(FPN)来输出比例丰富的特征图，然后将每个特征向量作为训练样本，由分类分支和回归分支进行进一步处理。这类框架成功的关键在于训练期间选择正样本并且假定真实边界框中心的区域能够提供目标信息，即不被其它物体遮挡。然而在工业零的部件所处的复杂环境中，目标经常受到严重遮挡，无法满足中心假设。另外，常规的目标检测算法没有考虑使用刚性目标的可见部分为边界框的预测提供信息。针对以上问题和挑战，本课题拟提出一种刚性感知检测方法来提高目标检测精度。首先，研究利用目标边界框中的所有像素来指导生成可见性(visibility)掩膜；然后，使用可见性掩膜指导训练期间对候选目标的抽样，使得网络由所有可见部分监督并且丢弃遮挡部分；最后，研究融合所有高可靠性候选局部预测，以产生更加鲁棒的检测结果。


\textbf{C、面向工业机械臂抓取基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计}
基于检测-估计-修正的6D姿态估计框架显示出了优良的性能，被近期提出的大多数方法所采用。采用一个单独的6D姿态修正网络可以放松对前一级估计网络的要求。同时基于CNN的6D姿态修正网络的输入为目标图像与在前一级估计网络得到的粗糙姿态下的渲染图像，这样的输入可以减轻姿态修正网络对数据的需求。目前大多数方法通过估计目标图像与渲染图像之间的对应关系来求解目标图像中物体的6D姿态。然而这种方法很强地依赖于物体的纹理特征，而对于工业零部件，大部分物体呈现出无纹理或者弱纹理的特征，因此这种方法应用于工业零部件上会有很强的局限性。因此对于无纹理或者弱纹理的工业零部件，物体的几何特征需要被有效地利用，然而现有的姿态修正方法很少探索物体的几何特征。
本课题拟提出一个基于物体几何特征的6D姿态修正方法。在现有的基于纹理特征的对应关系估计网络中，如RAFT，本文通过利用点云特征提取网络，显式地将物体的几何特征进行编码，作为对应关系估计网络的参考信息。在迭代优化过程中，对应关系估计网络全局地通过可微姿态求解层来修正在上一步迭代过程中求解得到的姿态。具体来说，将物体的几何特征在上一步估计得到的姿态下进行投影，同时将物体的几何信息引入目标图像与渲染图像的特征匹配过程中，结合特征匹配结果与投影得到的几何特征来全局地求解更新之后的目标物体的6D姿态。



\subsection{拟解决的关键科学问题}

\textbf{A. 面向危险设备智能装配零部件的多源融合表征}
如立论部分所述，在姿态估计中加入多种信息，理论上是有利于最终结果的评估，但是在实际处理中，各种模态信息具有各自的信息特征，将多种信息有效的对齐融合则是高效解决问题的关键。针对危险设备智能装配环境，引入光谱数据区分火工品装配危险区与非危险区。针对目标数据的差异性，制定高效鲁棒的目标特征提取策略，同时考虑多模态特征的信息对齐和融合。

\textbf{B. 针对危险设备智能装配零部件复杂环境下的3D目标检测}
如立论部分所述，复杂场景中的目标存在严重的遮挡问题，这使得专注于学习从真实边界框中心区域抽取的样本来预测边界参数的方法就不再适用，有效利用刚性目标对象的可见部分来获取目标边框的可靠估计是解决该问题的关键。本课题利用边界框中所有像素信息来获得目标可见部分的掩膜，取代了传统复杂的标注分割掩膜的可视化方法。针对目标中心遮挡情况下无法获取有效样本问题，在推理过程中融合所有高可靠性候选局部预测，以获得更准确的检测结果。

\textbf{C. 面向危险设备智能装配基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计}
如立论部分所述，对于弱纹理的物体，当前基于纹理特征的6D姿态修正方法是不适用的。课题组前期对此进行了实验分析。如图所示，对于图中的弱纹理物体，基于纹理特征估计的渲染图像到目标图像之间的2D对应关系较差，无法保持物体的形状，导致最终通过RANSAC-PnP估计得到的姿态精度较差。尽管图中所示的物体不属于工业零部件，但是其无纹理的特征与工业零部件是相同的。通过该例子引申出工业零部件的6D姿态修正难点所在，由于工业零部件的弱/无纹理特征，在对零部件拍摄得到的图像中，大部分物体上的 像素呈现出一致的纹理特征，不具有区分度。在这种情况下，通过上下文信息以及几何特征感知像素在物体几何模型上的位置，进而估计2D对应关系是可行的解决方案。


\NsfcSection{3}{拟采取的研究方案及可行性分析}{
（包括研究方法、技术路线、实验手段、关键技术等说明）；}


\subsection{拟采取的技术路线}
本课题研究的面向危险设备智能装配零部件六自由度姿态估计的总体方案示于图\ref{fig:sys_arch}。其中涉及的三个关键问题的详细方案和可行性分析分别阐述如下：

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{sys_arch.png}
    \end{overpic}
    \caption{本课题研究方案的总体框架
    }\label{fig:sys_arch}
\end{figure}

\textbf{A.面向危险设备智能装配零部件的多源融合表征设计方案及可行性分析}

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{feature_biflow_fusion.png}
    \end{overpic}
    \caption{特征级双向全流多源融合
    }\label{fig:feature_biflow_fusion}
\end{figure}

输入部分为三种模态数据：2D彩色图、2.5D深度图和光谱图像。根据数据表达的意义采取不同的处理手段：对于2D彩色图像，表达目标实时纹理颜色，直接提取二维语义特征；对于2.5D深度图片，表达目标上各点到相机的距离，表达为三维点云更充分保留空间信息，提取空间特征；对于光谱图像，在危险设备装配环境中能更加有效识别目标，作为辅助数据优化结果。整个目标姿态估计网络框架分为四个部分：融合UV输入、特征提取、特征共享和特征融合。

\textbf{B. 针对危险设备智能装配零部件复杂环境下的3D目标检测网络设计方案及可行性分析}（王庆元）
\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{visable_guided_3d_detect.jpg}
    \end{overpic}
    \caption{可见性引导的3D目标检测流程图
    }\label{fig:visable_guided_3d_detect}
\end{figure}

该关键问题的研究重点着眼于复杂环境下刚性目标可见部分引导检测。现有方法专注于学习仅从真实边界框中心区域抽取的样本来预测边界框参数，当目标中心被其他对象或场景元素遮挡时将无法提供精确的检测结果，对于复杂场景下工业零部件的目标检测并不适用。由于6D姿态估计中的目标对象通常是刚性的，这类目标的所有可见部分都可以为边界框的预测提供可靠的估计。因此本方案提出了一种刚性感知检测方法，具体包含以下几个部分：

首先，对真实刚性目标边界框进行裁剪来得到图像块，依据刚性目标的可见部分与边界部分像素之间的显著差异性，设计一种可见性距离函数并计算图像块内所有像素与边界距离来构建目标的可见性掩膜；然后使用主干网路对融合特征进行特征提取来生成特征图，依据每个特征单元对应所有像素的可见性距离来计算特征单元的可见性分数，设置可见性分数阈值对所有的特征单元进行筛选采样获得候选正单元；最后，研究特征单元预测框的置信度，并根据置信度融合所有的候选预测以获得更加鲁棒和精确的检测的结果。

一阶段目标检测器通常使用特征金字塔(FPN)输出多种比例的特征图，将特征图中的特征向量作为训练样本进行分类和回归处理。在检测训练期间，需要为每个注释的目标实例定义正样本和负样本，并使用正样本回归目标实例的边界框参数。因此这一框架的关键是在训练期间有效的选择正样本。本课题的前期预研中，在常规的目标数据集(COCO)和典型的6D目标位姿数据集(YCB-V)上，将真实目标掩膜内随机采样一定数量的正单元策略与两种基准策略进行评估对比，包括从真实边界框(FCOSv2)的中心区域采样固定数量的正单元，以及跨所有金字塔特征层级的自适应中心采样策略(ATSS)。图4显示了使用这些采样策略获得的不同局部预测的平均测试精度。对于常规的COCO数据集，无论在训练期间使用哪种采样策略，精度都会随着距离的增加而恶化。这因为COCO数据集中包括许多非刚体目标和具有相同目标类型的各种实例，使得基于中心区域采样的方法也可以获得可靠的边框预测结果。对于YCB-V数据集，由于大多数非中心区域特征没有参与训练，基于中心区域采样的策略的准确性也随着距离的增加而快速下降。然而，由于YCB-V目标的刚性，可见性引导(visibility guided)策略即使是对于那些远离中心区域的预测，也能取得更准确的局部预测。本课题将把该方法从2D边界框回归拓展到3D目标检测领域。

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{rigid_detect_analysis.png}
    \end{overpic}
    \caption{刚性检测分析
    }\label{fig:rigid_detect_analysis}
\end{figure}

我们展示了不同抽样策略的检验精度。在典型的一般对象数据集(COCO[29])和典型的6D目标位姿数据集(YCB-V)上的训练期间，不同的局部预测。图中显示了FCOSv2(Center)、ATSS(Center+)的结果，以及利用真实掩膜中的所有候选框(Visible)策略的结果。水平轴表示局部预测到边框中心的归一化距离。

\textbf{C. 面向危险设备智能装配基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计方案及可行性分析（海洋）}

本方案的关键问题在于对针对目标物体估计得到的不精确的6D姿态进行修正。利用该6D姿态在图像对物体进行定位，进而对目标物体所在的区域进行放大，同时减轻其他物体以及背景对姿态修正的影响。该方案包括几个部分，首先利用卷积神经网络对在不精确的6D姿态下渲染的初始图像以及目标图像进行特征提取，接下来迭代地进行6D姿态的修正，借助于RAFT中提出的相关性查找组件，计算在估计得到的初始图像与目标图像之间密集的2D对应关系（即光流）下的相似度。利用该相似度，本方案进一步估计对该对应关系的修正。为了将物体的几何信息引入特征匹配过程，本方案根据修正之后的密集对应关系，通过姿态估计子网络直接回归6D姿态。接下来本方案将物体的CAD模型在该6D姿态下进行投影，得到几何信息驱动的对应关系，以作用于下一次迭代中的相关性查找中。

随着RAFT的提出，基于RAFT中迭代修正框架在许多领域展现出了巨大的潜力。文献[10]进一步成功地将RAFT应用到6D姿态修正领域，并展现出了优良的性能。这是本项目分析可行性的重要依据。本项目在RAFT的基础上，将物体的几何信息引入特征匹配过程，使得匹配过程结合纹理信息与几何信息，不仅可以减小匹配过程中的搜索范围，同时几何信息的引入使得无/弱纹理物体的姿态修正更加鲁棒。

\begin{figure}[h]
	\centering
    \begin{overpic}[width=0.8\columnwidth]{geo_guided_6D_refine.png}
    \end{overpic}
    \caption{基于几何特征引导的6D姿态修正框架
    }\label{fig:geo_guided_6D_refine}
\end{figure}

\subsection{可行性分析}


\NsfcSection{4}{本项目的特色与创新之处；}{}

\textbf{A. 多源融合表征的特色与创新之处}
一方面，通过引入额外的UV数据保持深度图点特征三维空间位置的一致性，由此可通过参数共享的神经网络架构，在RGB和Depth特征提取阶段，通过全流双向融合各自互补特征。另一方面，通过引入光谱图像，针对危险设备智能装配零部件不同地光谱特征响应，可以更准确和鲁棒地提取装配零部件实例级目标特征。

\textbf{B. 3D目标检测的特色与创新之处}
引入刚性感知检测方法，提出了一种新颖的可见性图来取代掩膜标注，并利用可见性图指导网络在训练期间对可见区域样本采样；考虑同一实例的不同尺度给检测带来的影响，引入尺度感知器来对融合特征不同尺度的特征图进行处理，提高了模型的特征提取与利用能力，并结合2D特征图采样结果来指导3D检测框回归；融合所有高于置信度阈值的候选局部预测，从而产生更鲁棒的检测结果。

\textbf{C. 基于目标几何特征引导的6D位姿估计的特色与创新之处}
通过可微相对姿态求解层逐步优化估计的6D姿态，并将几何信息引入特征匹配过程，使得匹配得到的特征与物体的几何形状一致。进一步显式地使用深度神经网络编码几何特征作为姿态修正网络的参考信息。

\NsfcSection{5}{年度研究计划及预期研究结果}{
（包括拟组织的重要学术交流活动、国际合作与交流计划等）。}

\subsection{年度研究计划}

本课题研究期限从2023年1月到2026年12月。年度计划如下：
2023.01-2023.06		总体方案制定和论证
查阅计算机视觉领域和机器人领域国际知名期刊和会议的最新文献
开始对课题组的博士生和研究生进行python，pytorch以及三维视觉的入门培训
撰写算法设计书
2023.07-2023.12		三维视觉感知硬件平台搭建
搭建三维视觉硬件平台
搭建算法训练测试服务器平台，完善课题组原有服务器实验平台
2024.01-2024.06		多源融合表征建模研究
研究内容1：面向危险设备智能装配零部件的多源融合表征
拟参加国际会议学术交流一次，拟完成国际期刊论文1~2篇，专利1项
2024.07-2024.12		3D目标检测算法研究
研究内容2：针对危险设备智能装配零部件复杂环境下的3D目标检测
拟完成国际期刊论文1~2篇，专利1项
2025.01-2025.06		六自由度姿态估计算法研究
研究内容3：面向工业机械臂抓取基于零部件CAD模型几何特征引导的目标六自由度姿态估计网络设计
拟完成国际期刊论文1~2篇，专利1项
2025.07-2025.12		各算法模块的优化
多源融合，目标检测和六维位姿估计算法形成模块，并在软硬件平台上进行验证和完善
拟完成国际期刊论文1~2篇
2026.01-2026.08		各算法模块的联合
多源融合，目标检测和六维位姿估计算法整合成完整的针对危险设备智能装配零部件目标六维位姿估计系统，并在软硬件平台上进行测试和完善
拟完成国际期刊论文1~2篇
2026.09-2026.12		准备课题评审
整理研究成果
撰写课题总结报告

\subsection{预期研究成果}

本课题将提出适用于危险设备智能装配零部件六自由度姿态估计算法。并基于该理论的研究成果，在国际学术期刊、国际学术会议和国内一级刊物上发表论文6-8篇；申请专利1-2项；培养博士研究生1-2名，硕士研究生3-5名。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ContentDes{（二）研究基础与工作条件}


\NsfcSection{1}{研究基础}{
（与本项目相关的研究工作积累和已取得的研究工作成绩）；}

\subsection{工作基础1}

在6D位姿估计方面，在前期工作中，课题组积累了大量公开数据集，同时在计算机视觉顶级会议ECCV 2022举办的BOP 6D位姿估计挑战赛中获得了最佳单模型奖项，提出的方法可以同时处理单目RGB以及RGB-D图像，可扩展性较强。

\subsection{工作基础2}



\subsection{研究工作获奖}


\NsfcSection{2}{工作条件}{
（包括已具备的实验条件，尚缺少的实验条件和拟解决的途径，
包括利用国家实验室、
国家重点实验室和部门重点实验室等研究基地的计划与落实情况）；}


\myPara{经费和硬件条件方面}我们


\myPara{人员方面}我们

\myPara{国内外合作方面} 我们


\NsfcSection{3}{正在承担的与本项目相关的科研项目情况}{
（申请人和项目组主要参与者正在承担的与本项目相关的科研项目情况，
包括国家自然科学基金的项目和国家其他科技计划项目，
要注明项目的名称和编号、经费来源、起止年月、与本项目的关系及负责的内容等）；}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ContentDes{（三） 其他需要说明的问题}



\NsfcSection{1}{}{
申请人同年申请不同类型的国家自然科学基金项目情况
（列明同年申请的其他项目的项目类型、项目名称信息，
并说明与本项目之间的区别与联系）。}


\NsfcSection{2}{}{
具有高级专业技术职务（职称）的申请人或者主要参与者是否存在
同年申请或者参与申请国家自然科学基金项目的单位不一致的情况；
如存在上述情况，列明所涉及人员的姓名，
申请或参与申请的其他项目的项目类型、项目名称、单位名称、
上述人员在该项目中是申请人还是参与者，并说明单位不一致原因。}



\NsfcSection{3}{}{
具有高级专业技术职务（职称）的申请人或者主要参与者是否具有
高级专业技术职务（职称）的申请人或者主要参与者是否存在与正
在承担的国家自然科学基金项目的单位不一致的情况；如存在上述情况，
列明所涉及人员的姓名，正在承担项目的批准号、项目类型、项目名称、
单位名称、起止年月，并说明单位不一致原因。}


\NsfcSection{4}{}{其他。}

无


\end{document}
